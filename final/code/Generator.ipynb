{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e8140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HMILAB\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import re\n",
    "from IPython import display\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2cc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, \\\n",
    "Conv2D, Conv2DTranspose, Dense, Flatten, BatchNormalization\n",
    "from tensorflow.python.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6feb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1926ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a61f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5626eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary_path = './dictionary'\n",
    "# vocab = np.load(dictionary_path + '/vocab.npy')\n",
    "# print('there are {} vocabularies in total'.format(len(vocab)))\n",
    "\n",
    "# word2Id_dict = dict(np.load(dictionary_path + '/word2Id.npy'))\n",
    "# id2word_dict = dict(np.load(dictionary_path + '/id2Word.npy'))\n",
    "# print('Word to id mapping, for example: %s -> %s' % ('flower', word2Id_dict['flower']))\n",
    "# print('Id to word mapping, for example: %s -> %s' % ('1', id2word_dict['1']))\n",
    "# print('Tokens: <PAD>: %s; <RARE>: %s' % (word2Id_dict['<PAD>'], word2Id_dict['<RARE>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e02e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sent2IdList(line, MAX_SEQ_LENGTH=20):\n",
    "#     MAX_SEQ_LIMIT = MAX_SEQ_LENGTH\n",
    "#     padding = 0\n",
    "    \n",
    "#     # data preprocessing, remove all puntuation in the texts\n",
    "#     prep_line = re.sub('[%s]' % re.escape(string.punctuation), ' ', line.rstrip())\n",
    "#     prep_line = prep_line.replace('-', ' ')\n",
    "#     prep_line = prep_line.replace('-', ' ')\n",
    "#     prep_line = prep_line.replace('  ', ' ')\n",
    "#     prep_line = prep_line.replace('.', '')\n",
    "#     tokens = prep_line.split(' ')\n",
    "#     tokens = [\n",
    "#         tokens[i] for i in range(len(tokens))\n",
    "#         if tokens[i] != ' ' and tokens[i] != ''\n",
    "#     ]\n",
    "#     l = len(tokens)\n",
    "#     padding = MAX_SEQ_LIMIT - l\n",
    "    \n",
    "#     # make sure length of each text is equal to MAX_SEQ_LENGTH, and replace the less common word with <RARE> token\n",
    "#     for i in range(padding):\n",
    "#         tokens.append('<PAD>')\n",
    "#     line = [\n",
    "#         word2Id_dict[tokens[k]]\n",
    "#         if tokens[k] in word2Id_dict else word2Id_dict['<RARE>']\n",
    "#         for k in range(len(tokens))\n",
    "#     ]\n",
    "\n",
    "#     return line\n",
    "\n",
    "# text = \"the flower shown has yellow anther red pistil and bright red petals.\"\n",
    "# print(text)\n",
    "# print(sent2IdList(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50fdd93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = '../data/'\n",
    "# df = pd.read_pickle(data_path + '/text2ImgData.pkl')\n",
    "# num_training_sample = len(df)\n",
    "# n_images_train = num_training_sample\n",
    "# print('There are %d image in training data' % (n_images_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80de249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686de4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "hparas = {\n",
    "    'MAX_SEQ_LENGTH': 20,                     # maximum sequence length\n",
    "    'EMBED_DIM': 256,                         # word embedding dimension\n",
    "    'RNN_HIDDEN_SIZE': 128,                   # number of RNN neurons\n",
    "    'Z_DIM': 512,                             # random noise z dimension\n",
    "    'DENSE_DIM': 128,                         # number of neurons in dense layer\n",
    "    'IMAGE_SIZE': [64, 64, 3],                # render image size\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'LR': 0.0002,\n",
    "    'LR_DECAY': 0.5,\n",
    "    'BETA_1': 0.5,\n",
    "    'N_EPOCH': 10000,\n",
    "    'CHECKPOINTS_DIR': './checkpoints/demo',  # checkpoint path\n",
    "    'PRINT_FREQ': 5                          # printing frequency of loss\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e1fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this competition, you have to generate image in size 64x64x3\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_CHANNEL = 3\n",
    "\n",
    "def training_data_generator(caption, image_path):\n",
    "    # load in the image according to image path\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img.set_shape([None, None, 3])\n",
    "    img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "#     img = tf.image.random_flip_left_right(img)\n",
    "#     img = tf.image.random_flip_up_down(img)\n",
    "    \n",
    "#     if(random.random() < 0.5):\n",
    "#         img = tf.image.stateless_random_crop(img, (48, 48, 3), seed = (0,1))\n",
    "#         img = tf.image.resize(img, size=[IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    \n",
    "    img.set_shape([IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNEL])\n",
    "    \n",
    "#     caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    return img, caption\n",
    "\n",
    "def dataset_generator(filenames, batch_size, data_generator):\n",
    "    # load the training data into two NumPy arrays\n",
    "    image_path=[]\n",
    "    image_class=[]\n",
    "    with open(filenames) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            image_path.append(\"../data/fgvc-aircraft-2013b/data/images/\"+line.split(\" \")[0]+\".jpg\")\n",
    "            image_class.append(line.split(\" \")[1].split(\"\\n\")[0])\n",
    "    captions = image_class\n",
    "    caption = np.asarray(captions)\n",
    "    image_path = np.asarray(image_path)\n",
    "    \n",
    "    # assume that each row of `features` corresponds to the same row as `labels`.\n",
    "    assert caption.shape[0] == image_path.shape[0]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((captions, image_path))\n",
    "    dataset = dataset.map(training_data_generator, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(len(captions)).batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8e877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_generator(\"../data/fgvc-aircraft-2013b/data/images_manufacturer_trainval.txt\", hparas[\"BATCH_SIZE\"], training_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3928409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b921b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "print(caption.shape)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec966478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Canadair', shape=(), dtype=string)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABV1UlEQVR4nO29e5RdVYH1O/fjnFOVV+UBVBJJMH4iARHEAKEEW4VomqE2NFwbvdiizSdXOkFeXjXdCsqnBuVrQTQGsWnQbjAtfQcqtoCMIKHVhEeUT5Q2gkYTDZX4SuVZ55y997p/REur9pyYE4K7Us7fGDUGrLOyzlp7rb1XnVrzzBmFEAKMMcaYPzFx1R0wxhjz54k3IGOMMZXgDcgYY0wleAMyxhhTCd6AjDHGVII3IGOMMZXgDcgYY0wleAMyxhhTCd6AjDHGVII3IGOMMZWQPlsNL1u2DNdccw36+/tx7LHH4hOf+AROPPHEP/rviqLApk2bMHHiRERR9Gx1zxhjzLNECAHbt2/HzJkzEcdP8zknPAusWLEi1Ov18C//8i/h+9//fnjb294WJk+eHDZv3vxH/+3GjRsDAP/4xz/+8c8B/rNx48anfd5HIex/M9L58+fjhBNOwCc/+UkAez7VzJo1CxdddBHe8573PO2/HRgYwOTJk/GG//m/UK93DXutiPgHtv4BVl7QuuPqCS2PI16/Tpqu13gb6lK2szYtr9UatHywXe5LoT6sRrwvO7M6Le+qlfsYFRmtm4prsjPjv9HERYuWN9JyO3nO244TPp5IrNII/AXWehzza1IE3pcIOS3PQ3n8dfFLXlHwNhRpwue5COU1FEV8/bQLfk3i0KTlCbnmbdHtVlGj5SHtouUFeP2CzxB/U0HI1aIg5aqq+COLumcT8Q8CKY8Dv69Q8PUG8PdE4O/ZlZQnKYn4xNViXh7EPLP5yQp+b9bT8sVttwbx5c8twdatW9HT08PfBM/Cn+BarRbWrl2LJUuWDJXFcYwFCxZg9erVpfrNZhPN5u9vjO3btwMA6vUu1Bvdw+qqDSglu0QkNqBahxtQjW1Aog25l8e83/U6f4DkMdmAgpgq0XZNPGxrdbIB5WIDIv3Y07bagPh1qZENKK5iA0r49VabhNqAYrIB1fbXBpSqDYiscbEBQTyYWb8BIEnJNRfPzlDwdSU3oEhsQHTTHz0bUIif+QaUiA0oiA0oAv8FLogNqPYn3oAieX/rzzB/7Bhlv4sQfvnLXyLPc/T29g4r7+3tRX9/f6n+0qVL0dPTM/Qza9as/d0lY4wxo5DKVXBLlizBwMDA0M/GjRur7pIxxpg/Afv9T3AHHXQQkiTB5s2bh5Vv3rwZ06dPL9VvNBpoNMp/SoiigGjER+kiG6TvOamr/O8Hdos/k4m/5ag/lLAP4qk4M2iSsxsAqIk//bRb4iM6+1wc+Hi6+V8+0J3yPyFE5M9qcY33OxPvqc4YskL+HapUpP6UUYg/qxRi3lLRTpKU+5KJ9ROhs7VCmpbnSLn4G0cUi7MU8afjIifjFH82zsVtnUfjaDnYfIo10a3OOQu+3tpqnslaSVK+fv7wT/R/iFJXxWQ6i1zc4eLPknEs+i3Gz6aireqKtdLNOg6gke6i5TEdqHqm8LYRifNpMvxu8qd0AAjsKRnzPyeWqu1VrQ6o1+uYN28eVq5cOVRWFAVWrlyJvr6+/f12xhhjDlCele8BXXbZZTjvvPNw/PHH48QTT8R1112HnTt34q1vfeuz8XbGGGMOQJ6VDeicc87BL37xC1xxxRXo7+/Hi1/8Ytx9990lYYIxxpg/X541J4TFixdj8eLFz1bzxhhjDnAqV8EZY4z58+RZ+wT0TKmlCWojviCnvtPEvjyeii9kRUI5k4kvRgb2zW+hblHfQ93ZFF/0FF+6jEk7qVAItZq7aXldfqGz3Lj6cq76QmOX+OLZbogv6ZF2EqHgioT6KBcuC0XgX4yMibIrFq4RUnkmvr2YkN/bgvjmZpKKL78KdVwmv1zJ+ieuN28BEGrMweZO0oj41rtY5HEklGpiPBHK89YW9gspkx1CqEUBgHxpNxRK1Sa+cCocKdSXeTOm9BRqNzWermgbLY/Ex4SuhHxhXVTOxRei01R9Cbs8/7FYWey5rASx5TaNMcaYCvAGZIwxphK8ARljjKkEb0DGGGMqYdSKEEKRIYywlVBuGgU5oE7FYZzKRqoz22vww7usrdyjxcG/sHSpiVgH2kXRtjqHzXN+aB8RcUKtxi1aGg1+sWpi/Du384PemNh9CINfOW9RwsUGmVoT9LCcHziHgs+9mjcQl+NY2ioL128hfAhBzFtUHn+khA9iUTTbQlRRLwtt8rZwZmYKGQCJ6EssTK5qMbPfEgfombKXocUAsQVS1jpK85FlfPzK/icnDdXEtQriML8pYk4mELEBwB2rE+VgzxzPAbRFAkRK+q6uIRNwKcuiUpt7VcsYY4zZz3gDMsYYUwnegIwxxlSCNyBjjDGV4A3IGGNMJYxaFVwUlS0e6ilXYbSJAiV0aMXTEFY3g0Q5lAr1GvVLARCnwtZDNBPH5foqWb0lVCyBtAEAOQmsGty1g9YdP76blqt5mCBUc7ua5dS80FEEIBApgxmhMkuIgk0pnmoJ70ssrvouctFrda4kDDlXcNXFtcoDTxjMibJLWbSosL9EqLIKpnhS7ky5UEbWhL0MUQwCwK5sgLznBFo3E4qqmrDLCew9hRXNRBHItlNcQxUwOK5e7kuR8X63lHQz7aHFuwK3OerOyxZKsbg3E6Fgi4U6LiJrIhHP1CQiz6tcPSNHvP9e1TLGGGP2M96AjDHGVII3IGOMMZXgDcgYY0wleAMyxhhTCaNWBRdCQBihcFJecAlRtpF8qN82zF9otXbR8q56WQkWRCiVigJLhMqq0cUVTxFR4GSZ8DFri76I0Lg2EdTEQvKkrqHyhJrczd+TqeZaot9ZzkPTshYP3lNSsJzMswrUKoQxXRzzeWPjGblWf9894eMlrq0KDgMJCCtEIzLUL+NqqpCVveAKsSaSiKvacuGdloj5qROVYrsYFHVJKCSAWuBrIifvGYH3r9Xm7zme3Pd7GhJrhfRlp+h3l1CeZSJ0sSlt1cr+gHUVMKeUkUK9mBN/tyTm44nIsyaQUECGPwEZY4ypBG9AxhhjKsEbkDHGmErwBmSMMaYSvAEZY4yphFGrgiuKCEUxfH9U/m7MbyvPxNCEiicRfm05iQxMRd1CqKxU/VgkbrKEwTThvysEHhSKZlMkcZJ0xUwkTtbrIrFV+Okp77QJpLwtlEDbdwv/rFiokmSCaLmdSPiSKYht3m/fs3wNI7GulF9bEJLOJFLpl+WGlLeb+r0yjkWyKPHyymWipUj5FAoppdRjaZ5RwVV6YukjEn6CCVGpFjlvu6788XKusAvi2qZpefzjUt5GLNRxbXGtmm2hYGO+b8KrrhBGiLUGf4AkxJMxFfcsm+NoLz/b+BOQMcaYSvAGZIwxphK8ARljjKkEb0DGGGMqYRSLEHIUI06BpfVIRA7HxKFwoXLN2kLgQA76cpECFwn7EnV4F0XCviQpH1Lm4tC6IYLQAG4tlGXl3zlidVCsvI/Eqmm3RTgcCQ5Lxe8+QRxyN+q8fhG4dc/ItQMABRk7AEQiBE/arpBmIiEegLD5YSIJAAiiPlAeT1vYGaXChikWwoKCiHtSIhIAgEK0oeycgjgUp5dL1E2FkkPlusWk8Zq4JkxkBEB7JSXcPqtJwue6a2LuyVwCQK0mREy8J2BTIR5jaIhnEA3vA2giYSEenikZZ16odTzibfaqljHGGLOf8QZkjDGmErwBGWOMqQRvQMYYYyrBG5AxxphKGLUquDiOS5YvbaFYYcKhSKl1VDATUWoBQE7UVMKJBknM28hEv5WqpLu73I5UGYmgrYzYE/22pVKJsgpStisq3Kq7myuEWq1y6JdSe9WJpcmefyBsSpTlElH3NIUyJxU2MnkQ80b8jxoiCIwF4wFALFRmAWr85b6nQknHVwqQCcVTzNSOCVcXhpRbt2Tifsvboo9ElZXWeM9z0W8WRAkALXJpIzGXWcQtnpJoJy1Pxf1WEPufFusIgFoqVH1CeReT4DkAaBdlqx91/+TiedDKhHKVdFF9WklTct9LNefetWmMMcY8q3gDMsYYUwnegIwxxlSCNyBjjDGV4A3IGGNMJYxaFVw7yxCNCNBiwVnAHt+4MioJTKnghM8cbYa3nWc89CoXKqtUeF9l7bIaiKmGACATKpZUqPpoeJQQrCjfr3ZbKOyEyowpDFUAIDcJAzI6x0Aq6reIv1lNqMZ2CSVdiIQSjFyXTNxKQYSmMb8yAEIbBxRE2xYn/Hqra5XUuEpRJ++ViZRvoFjjUW0CLc+LsiqtKJRnogh0FLc4C0ZUitO6ClkLvN8qvDEl660QisZcXMNEyBcj8OfKOOZvJ0L9YqGOawlfy5T4UdZEByOiAGRltF97VcsYY4zZz3gDMsYYUwnegIwxxlSCNyBjjDGV4A3IGGNMJXSsgnvggQdwzTXXYO3atXjqqadwxx134Mwzzxx6PYSAK6+8Ep/5zGewdetWnHzyyVi+fDkOP/zwjt6nXWSIRqhzVDJiQcoT4Z2mUlWV4islyg+lqImFF1yi1CNCftZslZUsXUSV8tse0lLVRyIQoumhgEiaBZRFHCJmIAUAxIerEIq5ukiRTDKhAhS6sSwj7cS84+NqXAk0KBJec3LNlXotEiq4XKgxi8DnOZBrmwmftTgSib3Cly4nqrkQCwWgUEyGWMyb8tOLyDhJ+vCeDvL1mcXcIy0i6tJC3Se56LfwvAuxiBwl85MID7sgVotS+yVSSUmSk0PZdxEAmsKPUj2D2HOyJdSVNZJWzFJ2GR1/Atq5cyeOPfZYLFu2jL7+0Y9+FNdffz1uuOEGPPjggxg/fjwWLlyIwUF+YYwxxvx50vEnoNNPPx2nn346fS2EgOuuuw7vfe97ccYZZwAAPve5z6G3txdf/OIX8YY3vKH0b5rNJprN3//Gv23btk67ZIwx5gBkv54BrV+/Hv39/ViwYMFQWU9PD+bPn4/Vq1fTf7N06VL09PQM/cyaNWt/dskYY8woZb9uQP39/QCA3t7eYeW9vb1Dr41kyZIlGBgYGPrZuHHj/uySMcaYUUrlVjyNRgONBj/wNMYYM3bZrxvQ9OnTAQCbN2/GjBkzhso3b96MF7/4xR21FSFBNML/KpPKrrJiQ1g2IRZqnVQouJjvVyI8uIL4QJkKdU87Fz5uRMHXGuSqknpd+IHlPLkxZkob4UsmRFNI6kKR1hY+VEQdGAs1kZq3IuP1I+Ep102uS6sl0jmFtCsTfx9gfmjKpzAWHmlCdAklMUzI2s+VnV5QHnZ8fiKiIo0h0oeDUHRmfL0FsYjY7dYq+IAipSJV6k12jwsVJWpCWSvUeyopNi/K4w/i8ZoIhZgQmQHCCy4i91We8/UjLCPRqIvUY5aIKsz3WL/1WEa0uXfV9o45c+Zg+vTpWLly5VDZtm3b8OCDD6Kvr29/vpUxxpgDnI4/Ae3YsQNPPvnk0P+vX78ejz76KKZOnYrZs2fjkksuwQc/+EEcfvjhmDNnDt73vvdh5syZw74rZIwxxnS8AT3yyCN45StfOfT/l112GQDgvPPOwy233IJ3vetd2LlzJy644AJs3boVp5xyCu6++250dQkreGOMMX+WdLwBveIVr6DnIr8jiiJcddVVuOqqq55Rx4wxxoxtKlfBKYoiLwXNKasbdgDIbCr2wDfPQpx+syAnZZmhbC3UwWAqDvWYdU8sbW54cSStXsr/oBCWIbGwPlLjCeI9G7TvygKEH/6qyLSYzP2ezpCDdRFIp+ju5p/a21m5nbaw7WmrAEQlNhBuNCAH1yNFOr8jy4V1i2icOS6pQ2RlKwUhqFEWRW0yP3WxxgtxraQtELPiifm6qiV8jgshWkiE8CGplZ8ThehgJJ41ym0rCHEGu5cT8YxELOx/xLXNWOikCO8bzHeUylrNvXO+sRmpMcaYSvAGZIwxphK8ARljjKkEb0DGGGMqwRuQMcaYShi1KrhQhHKomrCMoeFRKjROSE2E4IvaTyg7HxU0ldZ4uJUKzWNWP7lQ37RZ8BqAOBXWQkzBpoL+hH9HUNYoSmFIrGvaQqmlrknW4qqa7u5xvH5Uvl7KQkkpBltMCQRuudTVELdSS6jDhMwsEyrAGlMgiaC2GlFuAtoWpyjKbVM7GwCZskSSNxAvZvdsniulGm+jJWyBmDwuFqpLCGVkJuYnSbkVT4s8b9T9kCR8rQQV+KaeH6Qsy7ltT6rsw3JlRVReK4W4ZzOixmsJpexI/AnIGGNMJXgDMsYYUwnegIwxxlSCNyBjjDGV4A3IGGNMJYxaFVwURSVvNaU+owoXofqoCXWY8plLSX3lxRqCUrfwf5ALDzY2LRnxHwO00ka9J1NCMU8pAPrXE+K1BWiFIQuqK0TjgSjmAKECA9AW6jgQXz4VppYXSjnEFU8RuYYspA4AYuVixwzYACDlqr4C5WuubNmYFxoA5IXygiuXF6JunIi1Iua+IJ5vABDR+VdKLa6Oi8UayonKTAhoEQvl2biGCl8TysiYqPrkmuCop4HKEchJX5SVIJRiUIbjkbaVeo8siafxqx6GPwEZY4ypBG9AxhhjKsEbkDHGmErwBmSMMaYSvAEZY4yphFGrgouTpJS+qBIdE6LASVTaaKx8soS6hfhKqeRT6bWlUjGFjCmjiYkqQVRpZ2S0ZqmkJq5rLqQs0stKKJ6Yn566hnI8QjUWCbVWIOMMxB8OACKhEIrEmqgRdZxK1K0L9V5T+MylifDfC+V1m4sU0noqvO0irupjvnRxqtKHhaIz48m0IR7P35Mo9WTaasaVjkXg6kXmS1cobzux3urid3PRDBKiUqyJZ1A75+OsifdUvol1cr2UH2WiOl4I9WJSvi5BJQrT54S94IwxxoxivAEZY4ypBG9AxhhjKsEbkDHGmEoYtSKEPXtjPKKE75c1chgXi2CzQhzSxeJQnB2msbCmPQixgdjmmZXGnlbK7aSqETGF6sAwkMPytjgQT0QQlhpnJA75WSCdsj6S9iriwF3bBXUQShbztiNxbWNSP1NtCAGKGL7sCwsrC7EIXVRrWazbPCofRAdxOB9HfE2kKTeMyWlsGpATa6k850KGhriXU9GXNrnHY7F+olgFzHH7n7q45l0JmR8hklAWT8h4/SD6ErHgQSV4EoKIprKyIvdKooIric+RfJyOwJ+AjDHGVII3IGOMMZXgDcgYY0wleAMyxhhTCd6AjDHGVMKoVcHV6xHqjeFSCmXVwaxe6jVuMZEHrigRgjQEpswREial/ChUaJzoS4Ry39uZCoHjb5oKxUqLqFuUIg3EXgQAMhHipX6foeozZYsTiWsrlEAQ9j/MMyVTaj+xrpQtEAtZY2sQAHLRRk1c8lAIeyayhhKRsqbCCxvinsjaZD4Trmprtfg8iGWIQim4yJoLwiqoJa6hUmWBKN6SlF+TNhs7gLqYTwhlGxPX5sriSUTPMdsvAEiEApatiVjYZymlayqErnlW7juzbAKAiNhkRUL9OBJ/AjLGGFMJ3oCMMcZUgjcgY4wxleANyBhjTCV4AzLGGFMJo1YFl8Q1JPFwiUYiZGYxUQMVuVBNKUWaUHgULMCOhGntaUN4cxX8MhfCm4yF44mmkQi1CQtkAwBiywYINVVUcIlMIVQ8yv4pJxKhXCh+Iqh5EypAqYQq16/XhXeYUAgp/7lavTyfRc4nKBKqJAiFFFvLABDI+OlcAkiUSlN4wcVE8ZaJBZfWuTouVoFn4n6jt7LyLxTXqhAheAlZiZEMDOTjKQredirGycIo1SVp5iIsU4x/d4urF1MyRyrUrpEKVWxL+LsRhW5Qfnqk37lUyg7Hn4CMMcZUgjcgY4wxleANyBhjTCV4AzLGGFMJ3oCMMcZUwuhVwUUFkmiEkkIopwLxD8uF+iYn6qjftkJLI2ISp9RrylNNKdKUmo4lrobAfbKUUitS3mTkPYNScEVcCSQCKqEEXyPVjID2dmsL9ZVSdgWRcBvFRDXGh6MVhsKDi6sdVWKrUiMqxRdvh41fKdXUeJQqKydyuqB8/VQb4r5S7bDbLVJzKTwTY6ZQBcCWcyD+ioBOjw1CXar8BLvr5fpN4S8J0e9YPIMKkcLKbn11DWMhmVT3D6teyLTVcv9E1XK/9q6aMcYYs3/xBmSMMaYSvAEZY4ypBG9AxhhjKqGjDWjp0qU44YQTMHHiRBxyyCE488wzsW7dumF1BgcHsWjRIkybNg0TJkzA2Wefjc2bN+/XThtjjDnw6UgFt2rVKixatAgnnHACsizDP/zDP+DVr341Hn/8cYwfPx4AcOmll+I///M/cfvtt6OnpweLFy/GWWedhW9+85ud9SwCMRdTnmVMNaaUXapc+E2xdkTbSsGkFE9q/w/E3yyWShiuPmoJdRwr1b5NymdNVBfUU+Ztp7zdVFSomDfhccVCNNUsiCXxNP+C1VRKR6GYVIGREfcmy4iXmVIdqmjeQiihmAdZnPJ5EMtK1i+EOi4i11YlgqZdE3jbyk+QTH6WiY4Lz8iYKDcBIE/4OLNsR6ksTYT3oLiBUqFcZd6QABCjLOvsro+jdVWSbSHaZs+sVKhCme9mEH53pTb3qtZvufvuu4f9/y233IJDDjkEa9euxV/8xV9gYGAAN910E2677TaceuqpAICbb74ZRx55JNasWYOTTjqpk7czxhgzhnlGZ0ADAwMAgKlTpwIA1q5di3a7jQULFgzVmTt3LmbPno3Vq1fTNprNJrZt2zbsxxhjzNhnnzegoihwySWX4OSTT8bRRx8NAOjv70e9XsfkyZOH1e3t7UV/fz9tZ+nSpejp6Rn6mTVr1r52yRhjzAHEPm9AixYtwve+9z2sWLHiGXVgyZIlGBgYGPrZuHHjM2rPGGPMgcE+WfEsXrwYX/nKV/DAAw/g0EMPHSqfPn06Wq0Wtm7dOuxT0ObNmzF9+nTaVqPRQKNRtpkp8j0/I0ppG1FSLk9F0FSm7G/a/JCOWaDkwkYmFkIG1W91KByTw8iMnao/DZEKu6PtdPp7CO+3smOhDjjizD4WgXSFCCULwholScpvkKS8jbY4oFbWKElablutiQjcQqnZFkoOsYZyYgkVJ9zSJRGH2U2R9ReRNd5siXBFcRCthBxK3MMO/5MaXxRMgLGnL3z8COU+FmK9MZuoPW3wNZGJhrK8vMrrOW/7w5ecRsvHdQu7KSEqOWT6tFLZeYuvpXW7urppOQo+P1FaXp/SKohNvlb2jGizA0IIWLx4Me644w7cd999mDNnzrDX582bh1qthpUrVw6VrVu3Dhs2bEBfX18nb2WMMWaM09EnoEWLFuG2227Dl770JUycOHHoXKenpwfd3d3o6enB+eefj8suuwxTp07FpEmTcNFFF6Gvr88KOGOMMcPoaANavnw5AOAVr3jFsPKbb74Zb3nLWwAA1157LeI4xtlnn41ms4mFCxfiU5/61H7prDHGmLFDRxuQ+nLnH9LV1YVly5Zh2bJl+9wpY4wxYx97wRljjKmEURtIF0WB2OZwVUlBlF2tjEt+glBypEINU5AwrFQFYclgN65iqdfVeMoKnCQRKjDhMCJVKOyyCGUgCmUVJJQzUnlXLsvEp+lEWJ1EYj6ZYhAATyUTqOC5PBeWNqQvqbCiaYs2VEBaJq55lJTVdOKSoFB3dQd2TsLhCJHwEKKWVZCuQAhMkSgUWXE+SMtzYSOTkjUUi7qJsH7KZKifsDPqKqtur3j7qbTuf3zpZlq+8s4v0fLtrSYtHySJd7fe+lVa95p/4W2DX1pEZJy5mGOm/pXqx5H/dq9qGWOMMfsZb0DGGGMqwRuQMcaYSvAGZIwxphK8ARljjKmEUauCK4o9P8MRfltEgRSJkDXlq5QLJRgNhxMKjzjmShv19SmljmPlKthLNi7UYWmN9F2oqaDUYcKXjrYNroKLM+HJJ/zxlHlcIby52KUVb4lUqMMiGRBWbnywrfqnAgOF2k+orCLyuyJbmwAQhC+d8tOL62WfsIJ4mwFaRaqLRXgjm2d1vYkCEAB7QOwpJvdyKtR7uUjYi4XqFIHPW6tZlpPtEvfmN7/OwzmLlAfvNYSsMYp3l8r+7tzX8jaISg8AdgkV3DF/cV6pLBPPoJzMg76Ph+NPQMYYYyrBG5AxxphK8AZkjDGmErwBGWOMqQRvQMYYYyph1KrgmlmOEA9XXURCrQSSaFkTfm00vQ9AEQmJFEEp6YJUZAnlnVCVxGScMYTCTKV2CtUPczSv17gnHVUqgSdoAkAukkUj4s3FVF1P956xmM9IXpfytY2Uh59KySUppHsaIu0IhZlCCAy1/xxRiMXEpxAAgph7JFwJxRRLymMwJmmwABBIAi2g55kqpyJ+vSPh15aKi8i8IXOiXAQAxOJ6K19HoXS95l2vK5Xt3M2VhNN6p9DyX6z8Fi2vdfM00+e++mJSyq9JTayVn/3gv2h5c9uuUlkkElvpI2gvkhMAfwIyxhhTEd6AjDHGVII3IGOMMZXgDcgYY0wleAMyxhhTCaNWBdfOcmCECq6WChUPMTPLMp4iGAsFVyoiIJlqTJmKKUWaTO0UQhEmwIkjpVRTvmzcPytnMZpKICQSWyOh3kuESpEpD1s5bztOxLUVnl1B1I/I0lb+VEUhFE8i5ZQl8yrRTy6UkUHIzKK68P1i7QjvvVx4h+VSMVgmZomleBolqlCZFQVXgjFBYhREGixRNAJAoa4hUZ1GYoKCeASqW/Y17W/z+k+W78NvLfogrfuyNm98U8rv2R2tF9DyjDzLEuXXRkuB3iNOoeWBPFMjlZCM8tj3No/Yn4CMMcZUgjcgY4wxleANyBhjTCV4AzLGGFMJo1eE0A7AiMNrdjC2h/JhZCLCugpxPNZu8/KUWHUUKqsqEyIE0RcVeBYTK5VEBGTlwlqo1eIiDGZrEgkbolaTHyDXhMBBiTBYX5QwQ9kZqYCwWNgcgVxDZv8CAEi4IEKF3YWUiBDUwX8sjn9Ft2mSHoAkKV/btliIOjCRlxf0gL4zi5qR9+rv+6IGyvrC50foT7SAgIX6ifUWSSVQOewNAI6dfCgtf3jRh0plP9k9kdad8fLTaHmy9hu0/Lmv+htavptaRfF7thDrUwlzYjKfibSsKpdlwsWq9D57V80YY4zZv3gDMsYYUwnegIwxxlSCNyBjjDGV4A3IGGNMJYxaFVwRdaGIhlvv5MJOI03Lvh6qbk2EWzE7iT39KKt1ImE7ElS4laifqFA2ohxSVi+RCp5TyiHSTi4UgOo920Lh0moL5R3ro7K/Eb8SRULZlQsrImYGEkQban5CfZxomygj1fwkIhiRSYfQmXozKbhSq1A2OkLZxhRPSgWXRuNpeYgHaTmEkhDEdidT1joi7C4V10oFPTLUPbhoGh/Ptm4+/iNu/1qpbE57O607YTxfV0/cytfKpm1K1Vi+LlnG21Zqv1gE7LH1rBTEBVHHiUdK+f33rpoxxhizf/EGZIwxphK8ARljjKkEb0DGGGMqwRuQMcaYShi1Krg8qSNOhnuOsWAzgKuyUuYHBa2EUh5cVGQllGdMlfLbN6XFufDVYio7rlQCggrrUh5PxINLXhPlBybkcW21nGjQmBqP6reonwilGpm4XASe5WL8UmVFxh/UshL9TlXYnZDThYEfl9ue+BxaNxHrE2J9RoNltZbyjYvBlY7/9I+vE/U5zDfwsZ/8itZdftsjtDwXYYQ3v7/snbZzF2/7vxa+npbP+Py/0vLzL76Ilm/69K2lsuf+j+fRugWm0PKpLzyRliuvRjCFbiJUvuoZlKlQP+L3KNpg6sVM3RAj8CcgY4wxleANyBhjTCV4AzLGGFMJ3oCMMcZUgjcgY4wxlTB6VXBRUkp2VP5u3Y2yYoOGBQLIAn9B5zYyRZrwXxPRjWki9nnhl5SS9MZI/K4Qi57XhJqsRRQrQSkGRRshVwoXocChXnDKr4u/Z1ITnmqildoO4uU1fhqtmwp/QBQ8gTci/mZFLnzwCt7vrpSP/8gjj6bls6dMLpU9Z+7JtO6k9Je0vNXm4+mZUG5b3Q9fvffrvO2MJ3FGQl06ZWI5LfTOz99C6775L99Cy084spuWr360rBic+zyuluyawst7hGLwhuv+Ny3fvXNXqWzCtENo3dtuvZeWP75VqEuVWjYm/oAiOVj5HRYRX58Re9ZKqSe5f6Sydjj+BGSMMaYSvAEZY4ypBG9AxhhjKsEbkDHGmEroSISwfPlyLF++HD/5yU8AAC984QtxxRVX4PTTTwcADA4O4vLLL8eKFSvQbDaxcOFCfOpTn0Jvb2/HHctDgnjEoVdNHETn5FA4Egf/gVjR7EHUJ4eRKgQuibjA4a/+ah4tf+nR/4OW19Lye6bComUwLx9+AvpgnZ1RKgshGdRGS4GUBFMBQCBzkQmVSCwCzLKCH3ILxyGkZI6yDoUPMRGD7IH0UXREXEJEcZ2WP7T2/9DyZrvc0OHTedtxmETLCxU+lpfbTsR98vhjq2j5jOkTaHkiLIfqtbJw6MGH7qJ1t+/YQsvf8427aXnr14eXyr752P9H6x7ziU/S8nWXLKblP+3nfflNWr7m59y1gtY94SV8fp5YKRZzTYieSIhmlnGhibpplaCqScIl1TMlSsqNR8ImqfT+e1Xrtxx66KG4+uqrsXbtWjzyyCM49dRTccYZZ+D73/8+AODSSy/FnXfeidtvvx2rVq3Cpk2bcNZZZ3XyFsYYY/5M6OgT0OteN9xw8EMf+hCWL1+ONWvW4NBDD8VNN92E2267DaeeeioA4Oabb8aRRx6JNWvW4KSTTtp/vTbGGHPAs89nQHmeY8WKFdi5cyf6+vqwdu1atNttLFiwYKjO3LlzMXv2bKxevVq202w2sW3btmE/xhhjxj4db0CPPfYYJkyYgEajgbe//e244447cNRRR6G/vx/1eh2TJ08eVr+3txf9/f2yvaVLl6Knp2foZ9asWR0PwhhjzIFHxxvQEUccgUcffRQPPvggLrzwQpx33nl4/PHH97kDS5YswcDAwNDPxo0b97ktY4wxBw4dW/HU63U8//nPBwDMmzcPDz/8MD7+8Y/jnHPOQavVwtatW4d9Ctq8eTOmTxdyHQCNRgMNYqUTIS7ZzxRCxZTEZXVcEAFuLJAN4LYWABAV5T36n67iwopodznYCwDeNP9MWn7Gt7mtyaq13yuVveTEI2jdbu4AgxALSxtisZEL9RoKda24GrHexVVWu5vlZUaD/gD0/+JHtHxmLw/3yoS852cbf14qmzCeW7dMPqiHlkc1rlS78sqPlMq6Rd1/XHIxLV9w6otpec8krhi9+2tl9dmOnVzx9HeLP0TLl33iclqeEGulQWF71Tf/eFr+6Y8vpeUQ9xWbtk0bNtOqv+rn98m4BrfRmTTj16WyLZufonULYZ/1nPPeSss/f+2NtPzYgfWlsm9fvoTW/cFRh9LyH/4XD8Gb8/KraDlI32OlVBPzUOR8DaW1cjupUHoOZmUVZa7knyN4xt8DKooCzWYT8+bNQ61Ww8qVK4deW7duHTZs2IC+vr5n+jbGGGPGGB19AlqyZAlOP/10zJ49G9u3b8dtt92G+++/H/fccw96enpw/vnn47LLLsPUqVMxadIkXHTRRejr67MCzhhjTImONqAtW7bgzW9+M5566in09PTgmGOOwT333INXvepVAIBrr70WcRzj7LPPHvZFVGOMMWYkHW1AN91009O+3tXVhWXLlmHZsmXPqFPGGGPGPvaCM8YYUwlRCELuUhHbtm1DT08P/vKCG1GrD1e5KGsuNgIVhJVEvJFI+KE9+a0vl8p2bSsrrADgW9/8Ki3fJcK6MqEEa+0u93F8g08T84MCAIjx14gH2S3/+k1a981v4h52OVEGAsATT/yClh9xxMHlNoT6qF7nbbeExRUioYwk86z8zeIav1ZtoQ5MSTBX0eZ1zzyTh8YNDPABTSDhcABw1z1lFdz2nbtp3QASxgcgl/Zc5bnIRGUVvPfud3PF16+38Hul2SyPP025urIh1G4//in/ykYjlD0WjzlxBq27+KKLaLnyTmsTjzQACO2yJ+NXPv5+WvfOJ/h6e+nrL6TlRc7VmxFRuqqneRBekur5we5PFbgZkYvVbu3GXf/8dgwMDGDSJO59B/gTkDHGmIrwBmSMMaYSvAEZY4ypBG9AxhhjKsEbkDHGmEro2AvuT0aBkjhHKdVAFE8RS60EEGU8QfTqd7+OlkeXLiyVNQd30Lq//uUALW8LpVYh+tgmnkvtQa5imTyh7KMHANsz7kv3yINln7mfrudxGVu3zaXll77z/6Xl3137Y1r+0KPlpMsH/+s7tO5z53K1Usi511oh/KlYIuq0qTy1sxa4+qrV4urF2rjybfPXZ7yS1oVIPp0wgZe3M65g2/hUWU22bSdXZA3u4utzkCi1ACAivl1toXYL6nqL1M7JU1Qacvk9VZpnvc6v1YtfPIWWv/K0l5XKdu3iY//avTy19Ikf/pCWb97ElZ4Hv+DNpbJxx1xG6570IvFsEs+DJOXXvNUizxWl/qWlQCaUkQm5f4LoX06MHffSCs6fgIwxxlSDNyBjjDGV4A3IGGNMJXgDMsYYUwnegIwxxlTC6FXBRQGIhqs/2plILSXKj66U6z4+fPmZtDxPuBLqlz8v+01t3baN1m0Ncm+uXcInLBappb/6RTldcfJkriaaOu4gWn7JP76Llp/1qnPLbU8te7UBwNq1a2n5Iw/9Ny3/y9fw3Kfrr/uXUllvL1e7/XyriGQv+LWNU+5l1T1xWqnsTa87R7TB5+c/7/0CLV/6vg+WytIaXz87d3BVm1JlTTmIz+f//ZZPlsqiTKijhL8XU7sBwK6Bh0tlaeAqSog1GwdeXgThNUbuZd6CFHYhCfzx9X/YuhUpvkn3ibR8/tl/S8tnHsb7UiPPj1woBmtCMZgL/z0lKEtTMv7AP1PkBVd0RhAyuKg8nkJ4IxbEgI6VMfwJyBhjTCV4AzLGGFMJ3oCMMcZUgjcgY4wxlTBqA+kWvu3TqNWHBzHFwoonScqHem85lR+s/3zgu7R8+Uf+jZb/5Gf9pbJ6QwSbibC7RGg98sDtTmLye0EQIXBFzO1LIhKatucf0DcU/RAhVuJANxJ9SZOylUoQYyeuHnvajrgdCwK3ImpMfUWprDvZQuvWo0dpeVITdjms6+KwvcnsUgC02vzaHnLosbR805aZ5UIREBYnYt4KLogYHPxlqSwRIoQocAFOyH/N+6Lss8jReij4fRLXeKDZqy64mpZnrfJcNJt87BAH67m4l1NpakOaFqqKIA7+CyESiSI+nzERHGQFv68gBBuRkH7kefle1hsFsQ5r7cZdN/4/DqQzxhgzOvEGZIwxphK8ARljjKkEb0DGGGMqwRuQMcaYShi1VjxRRCw4hAyjKMrKlLe+7Rpatx7zoKlISFYmjSurrDoXDgornohfftZ8UBYosZjCQiihiIgnVl4nwgREjT4WwW55RhRiQpVTCFVOFJSVCKe1dWWpTLgZSSWdStWqNcpXYMd2YXPDHYSkDdO2XzxGy3cOltWYjWgWbzvjirS8WQ61A4CEWPfUuvk1OWTOX9Ly58w9mpav+N9voeW/2VxW2V22/D5atxA2Mrt2iIA9NhVKkiZ+B1c2PxCKtILc4kqNKBx3UEuFjQ5RpAHcgiwS6r0oFv0WdkGsbanqY88rdblH4E9AxhhjKsEbkDHGmErwBmSMMaYSvAEZY4ypBG9AxhhjKmHUquBQoGQxFNe431bIyj5PL3vD+bTumjt5yFhjIlcI5VueKrd9wY20rvKCU2FQIsMMrXZZQlIT3mG5ME8rxO8W3cVvSmW7ogm0biIUdkEpZ8RyilLW905VObQYuVDN1eIu0g+u7Opu82v1lX9dJNour7cpU8oBeAAwbly5HwDw1FM/oeVxyscza1pZvrh94Pu0Lrr4wnrXu2+l5UcdM7FU1hLr6hMfuZSWD67jyrvz39lNy4txZZ+9QkxynPB5k2uFaCPjWP2urcp521nGJWxJQlRjQqOZivuq1ebPiUT0nXnHKYVuIgI61TUMRO0XkTHuaaRcJB6FJfwJyBhjTCV4AzLGGFMJ3oCMMcZUgjcgY4wxleANyBhjTCWMXhVcPgjkI1QXJAEQAM8oLLgJV99rz+ZtJLzt7636cqls13aerNlIuOJHea0N7uYKFKZ6GcyE2quuvOBEEmcypfx+wt9KFCubOURCwRWIWkfazymPOJE4Gkdc3ROIf1a7KVJYhULo5ed+lJZHUXmcRa7UR3xd9d+0mJa/8sz/RcubXVNLZd+49T207kte9FJavvqnP6DlK39aXrc1cT90vejttHw8LYWc6JitCbWuxFoJQqlWFOV/kKYilVj4rCk1mepLRJ5CkfKMFKGqqUj9zYnKF+D3VSyUd3lbjFM8JyJyX7GkWQCI9z4ktvxv9/2fGmOMMfuONyBjjDGV4A3IGGNMJXgDMsYYUwmjV4QQoZQ2lkTCdoaIExIVpqYCpQI/jJvSvaFU1l3nYgMlkpABbuIAndljpDVRV4SmdWJpI5qQRIk4dVSBVeTQnh2g7mlcHIqq6Dk1flI/rXf2+5YKKWSliTiJrSX88Bcx7/edQliQpmVLn9POu5bWVYFnTRFKlpKUQmnpIk7hAzn439MO70xK1lAmLlUQ933cwel3IayFZICbeE6oZVuQtaLWbCYEAUL3gSTh1kpsjtS8Kfsf1Rc2b40Gt0RqtcqCr6CUSiPwJyBjjDGV4A3IGGNMJXgDMsYYUwnegIwxxlSCNyBjjDGV8IxUcFdffTWWLFmCiy++GNdddx0AYHBwEJdffjlWrFiBZrOJhQsX4lOf+hR6e3s7ajuOykKhXCi7EmKzIQRpUtkUJdwG4znHXVwqE91AyLnSRime4ohffqbuUaqSSCiblP0PC+ZSykClMlJKG+XRw+PoRBvKAkX8rqSCr1h9pYRSOkUVYsYuOVP6AUBbKNKSiafx8q0P0PImkYgF8DUbIh5sFhG1GwAEcU8w1JqVokbRdjsvl7NQNwCIxHtGYo0XIy28oO2j1Nhjcc9KpSdZt0pJVxO2QMzOBwCynNuKMQWfujW1Jk2pF0k/Mr6uaBN7qazd509ADz/8MD796U/jmGOOGVZ+6aWX4s4778Ttt9+OVatWYdOmTTjrrLP29W2MMcaMUfZpA9qxYwfOPfdcfOYzn8GUKb83txwYGMBNN92Ej33sYzj11FMxb9483HzzzfjWt76FNWvW7LdOG2OMOfDZpw1o0aJFeM1rXoMFCxYMK1+7di3a7faw8rlz52L27NlYvXo1bavZbGLbtm3Dfowxxox9Oj4DWrFiBb797W/j4YcfLr3W39+Per2OyZMnDyvv7e1Ff38/bW/p0qX4wAc+0Gk3jDHGHOB09Alo48aNuPjii3Hrrbeiq6tsDbIvLFmyBAMDA0M/Gzdu3C/tGmOMGd109Alo7dq12LJlC17ykpcMleV5jgceeACf/OQncc8996DVamHr1q3DPgVt3rwZ06dPp202Gg3hMRQwUrsR17gnEgugkkNTNmYimCmm/mEqOYs3Hiu/qVgFapXbl4o0qeAS/lHMa01JyYS0Kc/EtRKqMSZWiqQuR8yb8IhTnnJRXFa8BaK8ArSPmwpIy8kLLGAN4MFeAPCys8/gjeP/oqUPr7qnVFYUQpUkZJpqrTBvP6UYVGo/5fkmPfxIH4NQuxVQ602oyVIyP23lDSn8G8V4lMqMKQxliKJQ2OVBmOGpkDkSgpgIVWwh1r6S9cUkpFHdPyBhmep+GElHG9Bpp52Gxx57bFjZW9/6VsydOxfvfve7MWvWLNRqNaxcuRJnn70neXTdunXYsGED+vr6OnkrY4wxY5yONqCJEyfi6KOPHlY2fvx4TJs2baj8/PPPx2WXXYapU6di0qRJuOiii9DX14eTTjpp//XaGGPMAc9+j2O49tprEccxzj777GFfRDXGGGP+kGe8Ad1///3D/r+rqwvLli3DsmXLnmnTxhhjxjD2gjPGGFMJozYRNU3HIa2NG1amlGoRUzEphYzyrBI+VExTojybCqjURaGykuqz8rtmbaHgqql+C+UQmfJIqNeydpO3IRJRxfRQv7pExD9mLa7sSmrc90z5bTHlkFJNQagR1TgT5vslFGY5UQjtqS881QquhDrxFQtKZTp1UinyhFcfU12K302VD6BSQBbK24/MhfKCy4SFXy4UeXHOPNLUHKv33HsvxT3tlxd/KJT6Vdz3QmWmPO+Ymk5db5mQzHuCnJoYirb3sozhT0DGGGMqwRuQMcaYSvAGZIwxphK8ARljjKkEb0DGGGMqYdSq4NpASQ3GlDMAkBA1SC7SSaVfmfIuItWp6g5AJFQvsVAfZblQqhGFWJJ0mAopfKioGCaIBE1lnKf8sISajLWTiahQpXaTCkgla2RtKNWU+D0sU/Iroh1SSkflA4hEGc3xYibiCuKaJEK9lytFGrmEsVJ6ylRZ9busUN6FssKyyUWXqNW6RRtqjZfLU+JtBuikVJmIKvVd5fFHyr+QeldqJaVUwZH6oumnSSvm5SlJbVVzz1R9LJWW4U9AxhhjKsEbkDHGmErwBmSMMaYSvAEZY4yphFErQggoEEYc+CVChMAO0pTYoCZD7cTpLzlwVsFMymFD2ZckKqiOHGirIKws49YtSaraLjcUxIGztApSIVYRFxDkxF5GiirE4a/QD+ggNNJ31UaUqAA3dV3K9WWgobKoESKZJBbXkAhWlCinEPOjD7PL5UogI4UpYqkw6yeAh+DlwsoqFyKZGPxeBrPFETeQtDNSFk9BCY3K71mIiyIP86WAgLfDRCXSmkpqrJTYhKqVaF12bZVJVPn9jTHGmArwBmSMMaYSvAEZY4ypBG9AxhhjKsEbkDHGmEoYtSo4hJgoTpTCo6xAiYVdjgqm2nuDjaez+REWKMJfhdldAEBB+qgsXWrCuibLBmk5U04xRRLwNDY3wmZDjp9cL2UBkmfcjyWJGrwvsVBCEaWR6p9SDqnhBzIX6jc5GUYoVpxaK6zrKmRM2aCkQhkJYkOVpCLoUKjGCmVbJNVx5QHFCZ9LZTkEoZqjQW0qkE2GRYpxCjVdToqDeE+ljFQXKwSudGWKvKIDpdrT1Wey20jNfV7uXyHUuSPxJyBjjDGV4A3IGGNMJXgDMsYYUwnegIwxxlSCNyBjjDGVMGpVcFFUViEVyreJ/XulKBEKlEQYhTH1iAr8ksq7vc9M+207xH9OyfSEiiUWXmtMgaOulRTIKHUYhPIlJsFZovEk4eFjifJUy4RCKuoi/RMKO3Gt8px7kBXEm0v/JifGqXwNxcWlnnfKl0z46cn7h6z9SJiHBfGe0mORvyN9oRC+fhAK0E6ueRCSRqWwU/eygs1nJFSHuVCISYWu8J5M60QBK3zmRP4jAvFpBMQlF6F+bLnFMrhwRL29qmWMMcbsZ7wBGWOMqQRvQMYYYyrBG5AxxphK8AZkjDGmEkatCi6J45LaTKl4EuIhpRQlOfEtArTvGStXflBBpEhKhPcVjT/dewHgb5vg48nJNazVuLoly7gKLCKqNgAIuVAYkmLVP6niKXj9RP0OFZX7rtI580zNPZ+flIw/F0olpVTLhddYEOmfLOFVpd5mbd6XhE0EAG5Zpvz+aDEyoSaLOlDqxWpNSFM+cc1J2xEza4O+rVQCsXpO0GeCuH9U20peGteFRx4ZUy7mQSl3a8RHEwAicr8NiudvIKo+VsbwJyBjjDGV4A3IGGNMJXgDMsYYUwnegIwxxlTCqBUhFEVRCpFSB2nM1iURYW95mx9yq3NOZskRZIAZf0/pxCNOI2nwk7BuEa4rWoRBeqOEGUpS0emiCQW7hkqwoK4Jb7sQc8EOtPOg7FV4eF8iRAg5DQzszIZJdBtJogIGWaifOLSWLjL8mkcxmWlx4BwpFQKxJwL0fVWQw3KpNZCn9kLgwMLU1I0iAg0LJVpQh/bkPQu13oTQRln3ZG0lnGIpheJaxfw9p3Xz61JPy9flN7v4ffJrMvfK8qzUrb2qZYwxxuxnvAEZY4ypBG9AxhhjKsEbkDHGmErwBmSMMaYSRq8KLoSSCk4FUyVM4cL9RbSKR/WDqHWStKFq01Kl4mEWQgC3BylEcBSE8k4poVIiywpC71ZTKjChmksSEfpFJGyiqnRXUX2MhRKMqp5EwFwsrmFp/f1Bb0ptiDspE4F5SpWVC8shVl/awqhux/yFmLSTqUAxYTelxyPUdMxtSgi41NpPpdKVqBTVghMXK5HyPd7JnNSPlFpW9CUq+PpsyLxI0ndRtyvhYYzN9t57fNWEldOktNyPtlARjsSfgIwxxlSCNyBjjDGV4A3IGGNMJXgDMsYYUwnegIwxxlRCRyq497///fjABz4wrOyII47AD37wAwDA4OAgLr/8cqxYsQLNZhMLFy7Epz71KfT29nbcsSiKSiFcVO22p3apJMuEukWEr6mdmIXgxULdolRTSnmnAt/itNzHWLQRCZM0pexqEXVTJJRkiRinUh9pxy5yzZVMT6ivlDpOThxR+8XCZ06Fxkm1UiABXCowL+bebkrZpeaZBt6JS5iIa6uUhCB9V+F1rC4A5IHPm2qHeTIqr8dYSAyZQhUAEuLX1hBzrzz8UhEMGMTzYzAvX5e28PWLyfoBgHabj6cIfPx14h2nwiJz8TxMhf9co1YeT13cD+PIMJv1Z8kL7oUvfCGeeuqpoZ9vfOMbQ69deumluPPOO3H77bdj1apV2LRpE84666xO38IYY8yfAR1/DyhNU0yfPr1UPjAwgJtuugm33XYbTj31VADAzTffjCOPPBJr1qzBSSedRNtrNptoNn+vUd+2bVunXTLGGHMA0vEnoCeeeAIzZ87E8573PJx77rnYsGEDAGDt2rVot9tYsGDBUN25c+di9uzZWL16tWxv6dKl6OnpGfqZNWvWPgzDGGPMgUZHG9D8+fNxyy234O6778by5cuxfv16vOxlL8P27dvR39+Per2OyZMnD/s3vb296O/vl20uWbIEAwMDQz8bN27cp4EYY4w5sOjoT3Cnn3760H8fc8wxmD9/Pg477DB84QtfQHd39z51oNFooNFQ1jbGGGPGKs/IC27y5Ml4wQtegCeffBKvetWr0Gq1sHXr1mGfgjZv3kzPjP4oIZSkT0olUq+XNzCl1NIRlUolUvZDy4T6Ron0CqZgApDURIIq82sT/R6pFPxj9UE8moJQtxRCehYJ1Ziy2eskuDETaqq0xhVFkZgLmhYpPvDnQpVElWcAUpqSq9SVQpWk0jxpKRAnpB2xZjPihQYAtbpIFGb+bjLhVJn4CR9A6QlGvNOkMlKo3XKe0BkX5fKQiX4LlZ5KJ+0S42+kJIVVzE8rF/esULAJ4R2iuLxudw6KZ5MYT00oD5k6ri7mh62fNJexvMP7tVe1BDt27MCPfvQjzJgxA/PmzUOtVsPKlSuHXl+3bh02bNiAvr6+Z/I2xhhjxiAdfQJ65zvfide97nU47LDDsGnTJlx55ZVIkgRvfOMb0dPTg/PPPx+XXXYZpk6dikmTJuGiiy5CX1+fVMAZY4z586WjDehnP/sZ3vjGN+JXv/oVDj74YJxyyilYs2YNDj74YADAtddeiziOcfbZZw/7Iqoxxhgzko42oBUrVjzt611dXVi2bBmWLVv2jDpljDFm7GMvOGOMMZUwahNRkzgqJWyqtEgWDFgEngCYpEJNJdMliXea8skSVlsqjTESZl458QlTiZNQ/RZqsnq9PP5MJIVKZZNQzijarfJcxBGfh1RI6VRaZiSXMOu7UCMKdY8IEKVz0RZplpH4Ha+m0jJFH1mp9PAT81MIP7CY1RcXXPmvpcS/cM8/UGnA5etSiKTdWKSwxkF5wZXbVgmnKt1XeSnmQtnGFIbSe1Cl+4o10W7v4u8Zd5XKJnUpNaJYhzXhkUfUm1HMnyntVnk8bbFOyu9jjDHGVIA3IGOMMZXgDcgYY0wleAMyxhhTCaNWhBBCgTDCCiZWVh3k8C6OyxY6gA6NkzYy5EAzFoe/hQyHEwfrIpSMnZeyg9XftiLaFrVJ42nCrxW7roAOU4PoI7MzUgIHZdEDcQAKcDEDO+jNhdiiJg7QcyFk6crKbUfxOFo3EzY/qViHmbiGBV2HYn7E4bwKX0N7R6moJkQvRcbnoYXxvG0xn3FRvraJCgZUh/ZKsEItenjdRDwClSCiKcaTEnGTnAfehHxO1IStGJv/IN5T2Wpt38HtjAIRodSEdVhM7uVWk987pX+7V7WMMcaY/Yw3IGOMMZXgDcgYY0wleAMyxhhTCd6AjDHGVMKoVcFFUVxSoAWl+KKFynZEWb0oSwrWN2GBkgobDCHtUplxdZJApcL4EmFpk5CALAAoit2lskjYjkCpkpSqT6gD2fiVm1Ei+iJEfTL0i1koJcIuJ22JtSLsRAqUFVIN8LbrYo6VgqsQAW67k4mlskgoPWNxTZSarMZUlzlXMakAwARlWxgAKIQKEEQFGJRyNS+vWQCIhEozTct9SUXdOFGhfiK8UNgCBXKvBGGHlYs55uo9ba3EbMUiYbnDwusAoFHn91u7We57ECrSgliKsfuP4U9AxhhjKsEbkDHGmErwBmSMMaYSvAEZY4ypBG9AxhhjKmHUquDyIi8FUaXKi4j5VimvMaFAUaospo7LMhHgJgPCuLpHhWS1iQ9VmjZ4/4SyKagwuaKsnFH5crkKZBPKLmX8xYO5eONK6aiuVUZ82QD+m1Us/L3awtuuIRWT5fIoEt5XYr3FJEwM0EGCXdmvynWzblo3EwokpXbMiPqMjRHQwYi1bCdvW4QuMt+zEAnVmAqRFH567Wa5L0HcP3Xl6yjmQS39Vqv8TOjq4mNPxTVU41SfEth7BqUKFSpa5ZmZkHtW5Bmi1Spf71aLK/pK779XtYwxxpj9jDcgY4wxleANyBhjTCV4AzLGGFMJ3oCMMcZUwqhVwcVRXPLLkimfRAmm/MqUdxgirgaJiFpLJpyqNE+l+BIecXFcnpZcJFGqlNhCpUgm5D0LPh6VuCmvLS0VKY3Key/jarKCXBMASJXyjvhWtROuMqoT7zAAyIQHGVtvdaH2UnNfCK+1WszVWixZNYhrFQm/srjB+xhQXlu5UEch5+swEemxtYgrDJmlXBDrKq7ztNlc/f5M6rcykUwr1H5B3BNK6ZkRhWWR8zuC3d8AEIt7NhP3Phu/SrJNxHxG6hqSdtqDwgcwKddNRD9Kb7NXtYwxxpj9jDcgY4wxleANyBhjTCV4AzLGGFMJ3oCMMcZUwqhVwQWEkg+bShikXmsiuVFptZT3E1OZKS80pUhTypkgGmKKtzTlKpZcJVSKxM2IjV8khUIlpQoPrlzIFBP2npHwsBO+eUrZhUwo1Ug7NaEyKtq7aHmaCh8z8nubTL0V1yQCH2db+AwWgfRFpcoKJV0Iykux3HelOsxF26mKrBUpmkwZGYn7pBCmhEEloiak78IHMBf9i2KVlKrms7zGd+3mysAJ44XyTj6yeH2meFOeiYVQYybiGjLhXVdDPINy8owUz4iR+BOQMcaYSvAGZIwxphK8ARljjKkEb0DGGGMqYdSKEBK0kYzoXhpzK5Up3eWDtyjih6W/EgeDhTjMz8hhWir27UgcAAZx4J5EIoCKHQqL/sWpeE9hdcPOHOviILKtgrOUhZAQeARWLg7EpWBDWXvUJtDiiF1zZWeU8mC3tnhLNhcpxMF/wg+/E2XnJCyhWFhbVJ/IO6iuoRByZExoI6xokpS3MbibHzqnKZ9nZomlxD2JGA9EkCDribKoiVXooBI+KKEEsfRhoW4AkImQQuKItKedmgjeY/ehCKJU15Y93wCgKMrzFiIlKCl3nNpvEfwJyBhjTCV4AzLGGFMJ3oCMMcZUgjcgY4wxleANyBhjTCWMWhXc9El11BvDVWLdda4aY7YzhQjOyjKhQBGqsYEm26NV2JJQganAN6HMYYov5fLTbnG1Sb0uLG2IHCYIVY66VolQDimlXpKUO68sagqhykmYvcrT1i+Pv4iFFY/otwreY3ORCWUgwNds0aGd09R6uf20xud4y8AO0TjvS8RUkCRkDAAyIW6qdXM1Yi6UailVRgqVYhB2OeKeTYnyMInEDSTmXtlkNWoiGJE0nwslnbxPhGquLe5PZsNVKPWiuGdV6GSbBCYmIsyTLX15O5Te3xhjjKkAb0DGGGMqwRuQMcaYSvAGZIwxphI63oB+/vOf401vehOmTZuG7u5uvOhFL8Ijjzwy9HoIAVdccQVmzJiB7u5uLFiwAE888cR+7bQxxpgDn45UcL/5zW9w8skn45WvfCXuuusuHHzwwXjiiScwZcqUoTof/ehHcf311+Ozn/0s5syZg/e9731YuHAhHn/8cXR1de31e8VRQVQXwt+MlAvRFCZ28z13d5OrZCYQH6admQhbSoRfm/BaK4SKJyYKnFzIj1RQXSFkKExpk4vguS4RytUUw49FCB5T/SjfPKU8k+MhCrs99cudDEJ9VBPKs1S0zUK82k3hSyaC3dKEj2fKBO5LN31KWWWWC6XnxG6udlvXv42WM0Gast5TKj1lb5aKNZEkZe+8mlCqDe7igYGFkBLWmamaCkAMfN7ErYlY3BNtGlQn7nsZlslpZ7yPzOAtEuOJRQBiofwriSKRes8BCHm5DdXlkXS0AX3kIx/BrFmzcPPNNw+VzZkz5/cdCQHXXXcd3vve9+KMM84AAHzuc59Db28vvvjFL+INb3hDJ29njDFmDNPRn+C+/OUv4/jjj8frX/96HHLIITjuuOPwmc98Zuj19evXo7+/HwsWLBgq6+npwfz587F69WraZrPZxLZt24b9GGOMGft0tAH9+Mc/xvLly3H44YfjnnvuwYUXXoh3vOMd+OxnPwsA6O/vBwD09vYO+3e9vb1Dr41k6dKl6OnpGfqZNWvWvozDGGPMAUZHG1BRFHjJS16CD3/4wzjuuONwwQUX4G1vextuuOGGfe7AkiVLMDAwMPSzcePGfW7LGGPMgUNHG9CMGTNw1FFHDSs78sgjsWHDBgDA9OnTAQCbN28eVmfz5s1Dr42k0Whg0qRJw36MMcaMfToSIZx88slYt27dsLIf/vCHOOywwwDsESRMnz4dK1euxItf/GIAwLZt2/Dggw/iwgsv7KhjcRQjHqGIUmqytFbeR3fvHuR1heIrEXGEUyaUFUXjRbLktl1CgSKUQ7tUoiMR1KQ1nrgJkTyYgLedF+V2mKoL0P2WaZ4qhZUooSKVmCjaVu57E8jcA8DU7rLqp9EQ6alC8aQUefV6uTwV3mkigBeJUB+NG88VbBO7y/O2q7Wb1m3U+Rp/kUgn/e+fDZTKglB0qvmJC37/1MR1qbGEV9G2CkSNlTcZ8WCLheqQ56dqL7hmk09oRBR81GMPANrKe1ApOnkzLHVUqReL1t73GwAKorotxF1YkOdYu+D+fSPpaAO69NJL8dKXvhQf/vCH8Td/8zd46KGHcOONN+LGG28EsEdae8kll+CDH/wgDj/88CEZ9syZM3HmmWd28lbGGGPGOB1tQCeccALuuOMOLFmyBFdddRXmzJmD6667Dueee+5QnXe9613YuXMnLrjgAmzduhWnnHIK7r777o6+A2SMMWbs03Ecw2tf+1q89rWvla9HUYSrrroKV1111TPqmDHGmLGNveCMMcZUwqgNpGu1BktWE81BfhpXI8FcqThwzXIhFEhE28y6RligHDSB211spzYdwFRxiPqbVvmwrxDCh/HisHiKsBza1iwfXO8KIugv50KOqOD1p00UoV/kLLa7zkUVLRGwB2FpM2MKt66JSABXLeXXpNEQNiUZn+cW8Z2Z0OB/Yp4ykR/ctoSdU8y7QtetWD5oNIR4JOHX6vkzy9f8Z/2/pHXTGp/7tM7HGQdxcI1yHwvh5yP0AIiUNQwRz+Q5PxQXuYgI4OswV55DRPQjD/iFPRGEjY66AIV4ljESIQYJQnzFrq0aO7O9yoTQYiT+BGSMMaYSvAEZY4ypBG9AxhhjKsEbkDHGmErwBmSMMaYSRq0K7q9e8VyMHz9+WNlXv/pVWre9u6wG+cIdd9C69913Hy1ftmwZLX/93/5tqexLX/oSrfs/33o+LZ/f10fLVZjam//2vFLZpk3cTfwfrvxHWn777bfT8necf26pbPZzZtO6UmYllE3S0+bZRL4l6eP+6h9tRv0uJ66hQnkO0Wve4XhU26MFta4U+2M+O1k/T/eerO+y7h/r1HBu+zJ/7t1//zdKZe+89B207rjJU2h5LJSeAzt2lsq2bd9B6/7jpYtLZdleJtL5E5AxxphK8AZkjDGmErwBGWOMqQRvQMYYYyph1IkQfpcps2vXrtJrKosjIwdpzB4CAHbs4Adpqm1WX9Xdvn07LVd9iTrIHGm1uJWIes/BQW6jw+t3eli6fw5Xn11IZ57V/j2bAof91P6omh9Ghx2sYj47mZ/91D/2LASAFnlO7BDPg1xkeykRwo6d5fdUz04mOPjdM1llhP2OKPyxGn9ifvazn2HWrFlVd8MYY8wzZOPGjTj00EPl66NuAyqKAps2bcLEiROxfft2zJo1Cxs3bhzTUd3btm3zOMcIfw5jBDzOscb+HmcIAdu3b8fMmTMRK/NVjMI/wcVxPLRj/u5PVJMmTRrTk/87PM6xw5/DGAGPc6yxP8fZ09PzR+tYhGCMMaYSvAEZY4yphFG9ATUaDVx55ZVoNHh42VjB4xw7/DmMEfA4xxpVjXPUiRCMMcb8eTCqPwEZY4wZu3gDMsYYUwnegIwxxlSCNyBjjDGV4A3IGGNMJYzqDWjZsmV47nOfi66uLsyfPx8PPfRQ1V16RjzwwAN43eteh5kzZyKKInzxi18c9noIAVdccQVmzJiB7u5uLFiwAE888UQ1nd1Hli5dihNOOAETJ07EIYccgjPPPBPr1q0bVmdwcBCLFi3CtGnTMGHCBJx99tnYvHlzRT3eN5YvX45jjjlm6JvjfX19uOuuu4ZeHwtjHMnVV1+NKIpwySWXDJWNhXG+//3vRxRFw37mzp079PpYGOPv+PnPf443velNmDZtGrq7u/GiF70IjzzyyNDrf+pn0KjdgP793/8dl112Ga688kp8+9vfxrHHHouFCxdiy5YtVXdtn9m5cyeOPfZYGf/90Y9+FNdffz1uuOEGPPjggxg/fjwWLlwona1HI6tWrcKiRYuwZs0a3HvvvWi323j1q1+NnTt/H/F76aWX4s4778Ttt9+OVatWYdOmTTjrrLMq7HXnHHroobj66quxdu1aPPLIIzj11FNxxhln4Pvf/z6AsTHGP+Thhx/Gpz/9aRxzzDHDysfKOF/4whfiqaeeGvr5xjd+H3c9Vsb4m9/8BieffDJqtRruuusuPP744/inf/onTJny+7juP/kzKIxSTjzxxLBo0aKh/8/zPMycOTMsXbq0wl7tPwCEO+64Y+j/i6II06dPD9dcc81Q2datW0Oj0Qif//znK+jh/mHLli0BQFi1alUIYc+YarVauP3224fq/Pd//3cAEFavXl1VN/cLU6ZMCf/8z/885sa4ffv2cPjhh4d77703vPzlLw8XX3xxCGHszOWVV14Zjj32WPraWBljCCG8+93vDqeccop8vYpn0Kj8BNRqtbB27VosWLBgqCyOYyxYsACrV6+usGfPHuvXr0d/f/+wMff09GD+/PkH9JgHBgYAAFOnTgUArF27Fu12e9g4586di9mzZx+w48zzHCtWrMDOnTvR19c35sa4aNEivOY1rxk2HmBszeUTTzyBmTNn4nnPex7OPfdcbNiwAcDYGuOXv/xlHH/88Xj961+PQw45BMcddxw+85nPDL1exTNoVG5Av/zlL5HnOXp7e4eV9/b2or+/v6JePbv8blxjacxFUeCSSy7BySefjKOPPhrAnnHW63VMnjx5WN0DcZyPPfYYJkyYgEajgbe//e244447cNRRR42pMa5YsQLf/va3sXTp0tJrY2Wc8+fPxy233IK7774by5cvx/r16/Gyl70M27dvHzNjBIAf//jHWL58OQ4//HDcc889uPDCC/GOd7wDn/3sZwFU8wwadXEMZuywaNEifO973xv29/SxxBFHHIFHH30UAwMD+I//+A+cd955WLVqVdXd2m9s3LgRF198Me699150dXVV3Z1njdNPP33ov4855hjMnz8fhx12GL7whS+gu7u7wp7tX4qiwPHHH48Pf/jDAIDjjjsO3/ve93DDDTfgvPPOq6RPo/IT0EEHHYQkSUpKk82bN2P69OkV9erZ5XfjGitjXrx4Mb7yla/g61//+rBExOnTp6PVamHr1q3D6h+I46zX63j+85+PefPmYenSpTj22GPx8Y9/fMyMce3atdiyZQte8pKXIE1TpGmKVatW4frrr0eapujt7R0T4xzJ5MmT8YIXvABPPvnkmJlLAJgxYwaOOuqoYWVHHnnk0J8bq3gGjcoNqF6vY968eVi5cuVQWVEUWLlyJfr6+irs2bPHnDlzMH369GFj3rZtGx588MEDaswhBCxevBh33HEH7rvvPsyZM2fY6/PmzUOtVhs2znXr1mHDhg0H1DgZRVGg2WyOmTGedtppeOyxx/Doo48O/Rx//PE499xzh/57LIxzJDt27MCPfvQjzJgxY8zMJQCcfPLJpa9E/PCHP8Rhhx0GoKJn0LMibdgPrFixIjQajXDLLbeExx9/PFxwwQVh8uTJob+/v+qu7TPbt28P3/nOd8J3vvOdACB87GMfC9/5znfCT3/60xBCCFdffXWYPHly+NKXvhS++93vhjPOOCPMmTMn7N69u+Ke7z0XXnhh6OnpCffff3946qmnhn527do1VOftb397mD17drjvvvvCI488Evr6+kJfX1+Fve6c97znPWHVqlVh/fr14bvf/W54z3veE6IoCl/72tdCCGNjjIw/VMGFMDbGefnll4f7778/rF+/Pnzzm98MCxYsCAcddFDYsmVLCGFsjDGEEB566KGQpmn40Ic+FJ544olw6623hnHjxoV/+7d/G6rzp34GjdoNKIQQPvGJT4TZs2eHer0eTjzxxLBmzZqqu/SM+PrXvx4AlH7OO++8EMIeGeT73ve+0NvbGxqNRjjttNPCunXrqu10h7DxAQg333zzUJ3du3eHv//7vw9TpkwJ48aNC3/9138dnnrqqeo6vQ/83d/9XTjssMNCvV4PBx98cDjttNOGNp8QxsYYGSM3oLEwznPOOSfMmDEj1Ov18JznPCecc8454cknnxx6fSyM8Xfceeed4eijjw6NRiPMnTs33HjjjcNe/1M/g5wHZIwxphJG5RmQMcaYsY83IGOMMZXgDcgYY0wleAMyxhhTCd6AjDHGVII3IGOMMZXgDcgYY0wleAMyxhhTCd6AjDHGVII3IGOMMZXgDcgYY0wl/P/ZzbK0cjDQtwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(image.shape[0]):\n",
    "    print(caption[i])\n",
    "    plt.imshow(image[i])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984aaa2",
   "metadata": {},
   "source": [
    "# Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f761371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2sentence(id2word_dict, id_sent):\n",
    "    batch_sentence = []\n",
    "    for j in range(id_sent.shape[0]):\n",
    "        sent=\"\"\n",
    "        for i in range(len(id_sent[j])):\n",
    "            if id2word_dict[str(id_sent[j][i].numpy())]==\"<PAD>\":\n",
    "                continue\n",
    "            sent=sent+\" \"+(id2word_dict[str(id_sent[j][i].numpy())])\n",
    "        \n",
    "        batch_sentence.append(sent)\n",
    "    return batch_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56631dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', \n",
    "    do_lower_case=False,\n",
    "    do_basic_tokenize=False\n",
    ")\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dca5924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_str(caption):\n",
    "    str_list = []\n",
    "    for i in range(len(caption)):\n",
    "        \n",
    "        str_list.append(str(caption[i].numpy()))\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3de1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(caption):\n",
    "    string_list = convert_to_str(caption)\n",
    "    bert_inputs = bert_tokenizer(string_list, return_tensors=\"tf\", padding='max_length',max_length=30)\n",
    "    bert_outputs = bert_model(bert_inputs)\n",
    "    caption_embedding = bert_outputs.last_hidden_state[:,0]\n",
    "    return tf.convert_to_tensor(caption_embedding.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "872c21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embed = text_encoder(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a2acf",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4214c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10000\n",
    "LEAKY_ALPHA = 0.2\n",
    "def my_leaky_relu(tensor):\n",
    "    return tf.nn.leaky_relu(tensor, alpha=LEAKY_ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d81f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Generate fake image based on given text(hidden representation) and noise z\n",
    "    input: text and noise\n",
    "    output: fake image with size 64*64*3\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = Flatten()\n",
    "#         self.d1 = tf.keras.layers.Dense(self.hparas['DENSE_DIM'])\n",
    "#         self.d2 = tf.keras.layers.Dense(64*64*3)\n",
    "\n",
    "        self.d1 = Dense(self.hparas['DENSE_DIM'], activation=my_leaky_relu)\n",
    "        self.d2 = Dense(128*8*4*4)\n",
    "        self.BN0 = BatchNormalization()\n",
    "        \n",
    "        self.conv1 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN1 = BatchNormalization()\n",
    "        \n",
    "        self.conv2 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN2 = BatchNormalization()\n",
    "        \n",
    "        self.conv3 = Conv2D(\n",
    "            filters=128*8,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        \n",
    "        self.BN3 = BatchNormalization()\n",
    "        \n",
    "        self.conv4_T = Conv2DTranspose(\n",
    "            filters=128*4,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.BN4 = BatchNormalization()\n",
    "        \n",
    "        self.conv5 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN5 = BatchNormalization()\n",
    "        \n",
    "        self.conv6 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN6 = BatchNormalization()\n",
    "        \n",
    "        \n",
    "        self.conv7 = Conv2D(\n",
    "            filters=128*4,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.BN7 = BatchNormalization()\n",
    "        \n",
    "        self.conv8_T = Conv2DTranspose(\n",
    "            filters=128*2,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN8 = BatchNormalization()\n",
    "        \n",
    "        self.conv9_T = Conv2DTranspose(\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN9 = BatchNormalization()\n",
    "    \n",
    "        self.out = Conv2DTranspose(\n",
    "            filters=3,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "#             activation=tf.tanh\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, text, noise_z):\n",
    "\n",
    "        text = self.flatten(text)\n",
    "        x0 = self.d1(text)\n",
    "        x0 = tf.concat([noise_z, x0], axis=1)\n",
    "        x0 = self.d2(x0)\n",
    "        x0 = self.BN0(x0)\n",
    "        x0 = tf.reshape(x0, shape=[-1, 4, 4, 128*8])\n",
    "        \n",
    "        x1 = self.conv1(x0)\n",
    "        x1 = self.BN1(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.BN2(x1)\n",
    "        x1 = self.conv3(x1)\n",
    "        x1 = self.BN3(x1)\n",
    "        \n",
    "        x2 = tf.add(x0, x1)\n",
    "        x2 = self.conv4_T(x2)\n",
    "        x2 = self.BN4(x2)\n",
    "        x = self.conv5(x2)\n",
    "        x = self.BN5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.BN6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.BN7(x)\n",
    "        \n",
    "        x3 = tf.add(x2, x)\n",
    "        x3 = self.conv8_T(x3)\n",
    "        x3 = self.BN8(x3)\n",
    "        x3 = self.conv9_T(x3)\n",
    "        x3 = self.BN9(x3)\n",
    "        \n",
    "        logits = self.out(x3)\n",
    "        output = tf.nn.tanh(logits)\n",
    "\n",
    "\n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e00cea",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b665e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Differentiate the real and fake image\n",
    "    input: image and corresponding text\n",
    "    output: labels, the real image should be 1, while the fake should be 0\n",
    "    \"\"\"\n",
    "    def __init__(self, hparas):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hparas = hparas\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        \n",
    "        self.conv1 = Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        \n",
    "        self.conv2 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        \n",
    "        self.BN2 = BatchNormalization()\n",
    "        \n",
    "        self.conv3 = Conv2D(\n",
    "            filters=256,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        \n",
    "        self.BN3 = BatchNormalization()\n",
    "        \n",
    "        self.conv4 = Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[2, 2],\n",
    "            padding=\"same\",\n",
    "            activation=None\n",
    "        )\n",
    "        self.BN4 = BatchNormalization()\n",
    "        \n",
    "        self.conv5 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN5 = BatchNormalization()\n",
    "        \n",
    "        self.conv6 = Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN6 = BatchNormalization()\n",
    "        \n",
    "        self.conv7 = Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[3, 3],\n",
    "            strides=[1, 1],\n",
    "            padding=\"same\"\n",
    "        )\n",
    "        self.BN7 = BatchNormalization()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #text\n",
    "        \n",
    "        self.d1 = Dense(self.hparas['DENSE_DIM'], activation=my_leaky_relu)\n",
    "        \n",
    "        self.conv8 = Conv2D(\n",
    "            filters=512,\n",
    "            kernel_size=[1, 1],\n",
    "            strides=[1, 1],\n",
    "            padding=\"valid\",\n",
    "            activation=my_leaky_relu\n",
    "        )\n",
    "        self.BN8 = BatchNormalization()\n",
    "        \n",
    "        self.out = Conv2D(\n",
    "            filters=1,\n",
    "            kernel_size=[4, 4],\n",
    "            strides=[4, 4],\n",
    "            padding=\"valid\"\n",
    "        )\n",
    "    \n",
    "    def call(self, img, text):\n",
    "        \n",
    "        x0 = self.conv1(img)\n",
    "        x0 = self.conv2(x0)\n",
    "        x0 = self.BN2(x0)\n",
    "        x0 = self.conv3(x0)\n",
    "        x0 = self.BN3(x0)\n",
    "        x0 = self.conv4(x0)\n",
    "        x0 = self.BN4(x0)\n",
    "        \n",
    "        x = self.conv5(x0)\n",
    "        x = self.BN5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.BN6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.BN7(x)\n",
    "        \n",
    "        x1 = tf.add(x0, x)\n",
    "        \n",
    "        #text\n",
    "        x2 = self.d1(text)\n",
    "        x2 = tf.expand_dims(x2, axis=1)\n",
    "        x2 = tf.expand_dims(x2, axis=1)\n",
    "        \n",
    "        x2 = tf.tile(x2, multiples=[1, 4, 4, 1])\n",
    "        x3 = tf.concat(values=[x1, x2], axis=3)\n",
    "        x3 = self.conv8(x3)\n",
    "        x3 = self.BN8(x3)\n",
    "        \n",
    "        logits = self.out(x3)\n",
    "        output = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b02a8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(hparas)\n",
    "discriminator = Discriminator(hparas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "380606e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption  = next(iter(dataset))\n",
    "text_embed = text_encoder(caption)   \n",
    "noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "_, fake_image = generator(text_embed, noise)\n",
    "fake_logits, fake_output = discriminator(fake_image, text_embed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4763f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_weights(\"models/W_generator_bert_epoch1300.h5\")\n",
    "# discriminator.load_weights(\"models/W_discriminator_bert_epoch1300.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "396952e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def discriminator_loss(real_logits, fake_logits, mismatch_logits):\n",
    "\n",
    "    real_loss = tf.reduce_mean(cross_entropy(tf.ones_like(real_logits), real_logits))\n",
    "\n",
    "    fake_loss = tf.reduce_mean(cross_entropy(tf.zeros_like(fake_logits), fake_logits))\n",
    "    \n",
    "    dismatch_loss = tf.reduce_mean(cross_entropy(tf.zeros_like(mismatch_logits), mismatch_logits))\n",
    "    \n",
    "    total_loss = real_loss + fake_loss + dismatch_loss\n",
    "\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    # output value of fake image should be 0\n",
    "    return tf.reduce_mean(cross_entropy(tf.ones_like(fake_output), fake_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8960d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = tf.keras.optimizers.Adam(learning_rate = 1e-4, beta_1=hparas['BETA_1'])\n",
    "optimizer_d = tf.keras.optimizers.Adam(learning_rate = 3e-4, beta_1=hparas['BETA_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f92becb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"checkpoint_dir\"\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=optimizer_g,\n",
    "                                 discriminator_optimizer=optimizer_d,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory = checkpoint_dir, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ea9f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1b28a5390a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2898ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10\n",
    "\n",
    "@tf.function\n",
    "def WGTrain(real_image, caption, noise_decay):\n",
    "    #c1: true image\n",
    "#     z = tf.random.normal(BZ)\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "    \n",
    "    with tf.GradientTape() as tpg, tf.GradientTape() as inter_tpg:\n",
    "        text_embed = text_encoder(caption)\n",
    "        \n",
    "        #c0: fake image\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        alpha = tf.random.uniform([hparas['BATCH_SIZE'], 1, 1, 1])\n",
    "        interpolates = alpha * fake_image + (1.0 - alpha) * real_image\n",
    "        \n",
    "        fake_image = fake_image + noise_decay * tf.random.normal(fake_image.shape)\n",
    "        real_image = real_image + noise_decay * tf.random.normal(real_image.shape)\n",
    "        interpolates = interpolates + noise_decay * tf.random.normal(interpolates.shape)\n",
    "        \n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "        mismatch_logits, mismatch_output = discriminator(interpolates, text_embed)\n",
    "\n",
    "        gradient_penalty = inter_tpg.gradient(mismatch_logits,interpolates)\n",
    "        gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "        loss = fake_logits - real_logits + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "        ld = tf.reduce_mean(loss)\n",
    "        lg = - tf.reduce_mean(fake_logits)\n",
    "\n",
    "    gradient_g = tpg.gradient(lg, generator.trainable_variables)\n",
    "\n",
    "    optimizer_g.apply_gradients(zip(gradient_g, generator.trainable_variables))\n",
    "    \n",
    "    return lg, ld\n",
    "\n",
    "@tf.function\n",
    "def WDTrain(real_image, caption, noise_decay):\n",
    "    #c1: true image\n",
    "#     z = tf.random.normal(BZ)\n",
    "    noise = tf.random.normal(shape=[hparas['BATCH_SIZE'], hparas['Z_DIM']], mean=0.0, stddev=1.0)\n",
    "\n",
    "    with tf.GradientTape() as tpd, tf.GradientTape() as inter_tpg:\n",
    "        text_embed = text_encoder(caption)\n",
    "        \n",
    "        \n",
    "        #c0: fake image\n",
    "        _, fake_image = generator(text_embed, noise)\n",
    "        alpha = tf.random.uniform([hparas['BATCH_SIZE'], 1, 1, 1])\n",
    "        interpolates = alpha * fake_image + (1.0 - alpha) * real_image\n",
    "        \n",
    "        fake_image = fake_image + noise_decay * tf.random.normal(fake_image.shape)\n",
    "        real_image = real_image + noise_decay * tf.random.normal(real_image.shape)\n",
    "        interpolates = interpolates + noise_decay * tf.random.normal(interpolates.shape)\n",
    "        \n",
    "        real_logits, real_output = discriminator(real_image, text_embed)\n",
    "        fake_logits, fake_output = discriminator(fake_image, text_embed)\n",
    "        mismatch_logits, mismatch_output = discriminator(interpolates, text_embed)\n",
    "\n",
    "        gradient_penalty = inter_tpg.gradient(mismatch_logits,interpolates)\n",
    "        gradient_penalty = tf.sqrt(tf.reduce_sum(tf.math.square(gradient_penalty),axis=[1,2,3]))\n",
    "        loss = fake_logits - real_logits + 10. * tf.math.square((gradient_penalty - 1.))\n",
    "        ld = tf.reduce_mean(loss)\n",
    "        lg = - tf.reduce_mean(fake_logits)\n",
    "\n",
    "\n",
    "    gradient_d = tpd.gradient(ld, discriminator.trainable_variables)\n",
    "\n",
    "    optimizer_d.apply_gradients(zip(gradient_d, discriminator.trainable_variables))\n",
    "   \n",
    "    return lg, ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0a25a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(caption, noise):\n",
    "    text_embed = text_encoder(caption)\n",
    "    _, fake_image = generator(text_embed, noise)\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c153399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1], 3))\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w, :] = image\n",
    "    return img\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    # getting the pixel values between [0, 1] to save it\n",
    "    return plt.imsave(path, merge(images, size)*0.5 + 0.5)\n",
    "\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(images, size, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "098be86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(caption, batch_size):\n",
    "    caption = np.asarray(caption)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(caption)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dea35530",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = int(np.ceil(np.sqrt(hparas['BATCH_SIZE'])))\n",
    "sample_size = hparas['BATCH_SIZE']\n",
    "sample_seed = np.random.normal(loc=0.0, scale=1.0, size=(sample_size, hparas['Z_DIM'])).astype(np.float32)\n",
    "sample_sentence = [\"Boeing\"] * int(sample_size/ni) + \\\n",
    "                  [\"Airbus\"] * int(sample_size/ni) + \\\n",
    "                  [\"ATR\"] * int(sample_size/ni) + \\\n",
    "                  [\"Antonov\"] * int(sample_size/ni) + \\\n",
    "                  [\"British\"] * int(sample_size/ni) + \\\n",
    "                  [\"Beechcraft\"] * int(sample_size/ni) + \\\n",
    "                  [\"Lockheed\"] * int(sample_size/ni) +\\\n",
    "                  [\"Douglas\"] * int(sample_size/ni)\n",
    "\n",
    "\n",
    "sample_sentence = sample_generator(sample_sentence, hparas['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dedfc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_sentence))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51bc3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "WTrain = (\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WDTrain,\n",
    "    WGTrain\n",
    ")\n",
    "\n",
    "WCritic = len(WTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7176ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_g_loss=[]\n",
    "epoch_d_loss=[]\n",
    "def train(dataset, epochs):\n",
    "    # hidden state of RNN\n",
    "    steps_per_epoch = len(dataset)\n",
    "    ctr = 0\n",
    "    for epoch in range(hparas['N_EPOCH']):\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        g_total_loss = 0\n",
    "        d_total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "#         if epoch < 200:\n",
    "#             noise_decay = 1.0 / float(epoch+1)\n",
    "#         else:\n",
    "        noise_decay = 0.0\n",
    "        \n",
    "        for step, (image, caption) in enumerate(dataset):\n",
    "            if step%30==0:\n",
    "                print(step)\n",
    "            g_loss, d_loss = WTrain[ctr](image, caption, noise_decay)\n",
    "            ctr += 1\n",
    "            g_total_loss += g_loss\n",
    "            d_total_loss += d_loss\n",
    "            if ctr == WCritic : \n",
    "                ctr = 0\n",
    "        time_tuple = time.localtime()\n",
    "        time_string = time.strftime(\"%m/%d/%Y, %H:%M:%S\", time_tuple)\n",
    "        epoch_g_loss.append(g_total_loss/steps_per_epoch)\n",
    "        epoch_d_loss.append(d_total_loss/steps_per_epoch)\n",
    "        print(\"gen_loss: {:.4f}, disc_loss: {:.4f}\".format(g_total_loss/steps_per_epoch,d_total_loss/steps_per_epoch))\n",
    "        print('Time for epoch {} is {:.4f} sec'.format(epoch+1, time.time()-start))\n",
    "        if epoch % 15 == 0:\n",
    "            manager.save(checkpoint_number=epoch)\n",
    "        \n",
    "        \n",
    "#         visualization\n",
    "#         if (epoch + 1) % hparas['PRINT_FREQ'] == 0:\n",
    "        for caption in sample_sentence:\n",
    "            fake_image = test_step(caption, sample_seed)\n",
    "        save_images(fake_image, [ni, ni], 'samples/bert_demo/train_{:05d}.jpg'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79ad6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"samples/bert_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d6aee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.1610, disc_loss: -8.3999\n",
      "Time for epoch 1 is 20.3674 sec\n",
      "Epoch :  2\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.6948, disc_loss: -8.3604\n",
      "Time for epoch 2 is 19.1836 sec\n",
      "Epoch :  3\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7206, disc_loss: -8.3799\n",
      "Time for epoch 3 is 17.7104 sec\n",
      "Epoch :  4\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.5859, disc_loss: -8.3627\n",
      "Time for epoch 4 is 17.8773 sec\n",
      "Epoch :  5\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4543, disc_loss: -8.3866\n",
      "Time for epoch 5 is 17.8202 sec\n",
      "Epoch :  6\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.9446, disc_loss: -8.3815\n",
      "Time for epoch 6 is 17.7681 sec\n",
      "Epoch :  7\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.8210, disc_loss: -8.3697\n",
      "Time for epoch 7 is 17.7307 sec\n",
      "Epoch :  8\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.2166, disc_loss: -8.3998\n",
      "Time for epoch 8 is 17.8183 sec\n",
      "Epoch :  9\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.6209, disc_loss: -8.3319\n",
      "Time for epoch 9 is 17.7592 sec\n",
      "Epoch :  10\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.4340, disc_loss: -8.3624\n",
      "Time for epoch 10 is 18.1920 sec\n",
      "Epoch :  11\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.1439, disc_loss: -8.3426\n",
      "Time for epoch 11 is 18.0385 sec\n",
      "Epoch :  12\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.5363, disc_loss: -8.3326\n",
      "Time for epoch 12 is 18.2837 sec\n",
      "Epoch :  13\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.7978, disc_loss: -8.3588\n",
      "Time for epoch 13 is 18.0933 sec\n",
      "Epoch :  14\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4515, disc_loss: -8.3721\n",
      "Time for epoch 14 is 18.2326 sec\n",
      "Epoch :  15\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.2551, disc_loss: -8.3527\n",
      "Time for epoch 15 is 18.3495 sec\n",
      "Epoch :  16\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.9623, disc_loss: -8.4103\n",
      "Time for epoch 16 is 18.5474 sec\n",
      "Epoch :  17\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4975, disc_loss: -8.3893\n",
      "Time for epoch 17 is 18.5494 sec\n",
      "Epoch :  18\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 66.1746, disc_loss: -8.3322\n",
      "Time for epoch 18 is 18.5127 sec\n",
      "Epoch :  19\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 96.2467, disc_loss: -8.3518\n",
      "Time for epoch 19 is 18.4437 sec\n",
      "Epoch :  20\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 68.5628, disc_loss: -8.3207\n",
      "Time for epoch 20 is 18.5358 sec\n",
      "Epoch :  21\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 51.7039, disc_loss: -8.4053\n",
      "Time for epoch 21 is 18.5719 sec\n",
      "Epoch :  22\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 40.8569, disc_loss: -8.3772\n",
      "Time for epoch 22 is 20.3002 sec\n",
      "Epoch :  23\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 33.9406, disc_loss: -8.3635\n",
      "Time for epoch 23 is 21.8603 sec\n",
      "Epoch :  24\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 29.5906, disc_loss: -8.3805\n",
      "Time for epoch 24 is 20.3337 sec\n",
      "Epoch :  25\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 28.1147, disc_loss: -8.3942\n",
      "Time for epoch 25 is 20.4056 sec\n",
      "Epoch :  26\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 26.1872, disc_loss: -8.3943\n",
      "Time for epoch 26 is 20.1590 sec\n",
      "Epoch :  27\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 25.5099, disc_loss: -8.3333\n",
      "Time for epoch 27 is 20.2210 sec\n",
      "Epoch :  28\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 24.5989, disc_loss: -8.3481\n",
      "Time for epoch 28 is 18.5308 sec\n",
      "Epoch :  29\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 24.1851, disc_loss: -8.4056\n",
      "Time for epoch 29 is 18.7155 sec\n",
      "Epoch :  30\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 23.5377, disc_loss: -8.3833\n",
      "Time for epoch 30 is 18.5700 sec\n",
      "Epoch :  31\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 22.4425, disc_loss: -8.3533\n",
      "Time for epoch 31 is 18.8719 sec\n",
      "Epoch :  32\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.8226, disc_loss: -8.3544\n",
      "Time for epoch 32 is 21.5070 sec\n",
      "Epoch :  33\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.5028, disc_loss: -8.3678\n",
      "Time for epoch 33 is 18.9976 sec\n",
      "Epoch :  34\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.3719, disc_loss: -8.3514\n",
      "Time for epoch 34 is 18.9737 sec\n",
      "Epoch :  35\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.4781, disc_loss: -8.3535\n",
      "Time for epoch 35 is 20.2760 sec\n",
      "Epoch :  36\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.1338, disc_loss: -8.4079\n",
      "Time for epoch 36 is 20.1494 sec\n",
      "Epoch :  37\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.7736, disc_loss: -8.3808\n",
      "Time for epoch 37 is 19.2348 sec\n",
      "Epoch :  38\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4958, disc_loss: -8.3795\n",
      "Time for epoch 38 is 18.9012 sec\n",
      "Epoch :  39\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.3111, disc_loss: -8.3948\n",
      "Time for epoch 39 is 21.6788 sec\n",
      "Epoch :  40\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.2632, disc_loss: -8.4122\n",
      "Time for epoch 40 is 20.1209 sec\n",
      "Epoch :  41\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.8162, disc_loss: -8.4242\n",
      "Time for epoch 41 is 20.2130 sec\n",
      "Epoch :  42\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.2125, disc_loss: -8.4051\n",
      "Time for epoch 42 is 18.7007 sec\n",
      "Epoch :  43\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5662, disc_loss: -8.3919\n",
      "Time for epoch 43 is 21.7450 sec\n",
      "Epoch :  44\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4955, disc_loss: -8.4085\n",
      "Time for epoch 44 is 18.6029 sec\n",
      "Epoch :  45\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4787, disc_loss: -8.3817\n",
      "Time for epoch 45 is 18.6671 sec\n",
      "Epoch :  46\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7549, disc_loss: -8.3849\n",
      "Time for epoch 46 is 20.1956 sec\n",
      "Epoch :  47\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.7588, disc_loss: -8.3748\n",
      "Time for epoch 47 is 18.9752 sec\n",
      "Epoch :  48\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 43.0645, disc_loss: -8.3782\n",
      "Time for epoch 48 is 18.7281 sec\n",
      "Epoch :  49\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 45.1607, disc_loss: -8.3799\n",
      "Time for epoch 49 is 18.9770 sec\n",
      "Epoch :  50\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 25.9391, disc_loss: -8.3750\n",
      "Time for epoch 50 is 20.5229 sec\n",
      "Epoch :  51\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.5817, disc_loss: -8.4313\n",
      "Time for epoch 51 is 18.9977 sec\n",
      "Epoch :  52\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.6012, disc_loss: -8.3469\n",
      "Time for epoch 52 is 19.1741 sec\n",
      "Epoch :  53\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.0253, disc_loss: -8.3682\n",
      "Time for epoch 53 is 23.3032 sec\n",
      "Epoch :  54\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.9131, disc_loss: -8.3937\n",
      "Time for epoch 54 is 19.1834 sec\n",
      "Epoch :  55\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.2941, disc_loss: -8.3752\n",
      "Time for epoch 55 is 18.8090 sec\n",
      "Epoch :  56\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5508, disc_loss: -8.3776\n",
      "Time for epoch 56 is 18.7683 sec\n",
      "Epoch :  57\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7703, disc_loss: -8.3775\n",
      "Time for epoch 57 is 18.7901 sec\n",
      "Epoch :  58\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.9478, disc_loss: -8.3668\n",
      "Time for epoch 58 is 18.9477 sec\n",
      "Epoch :  59\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4209, disc_loss: -8.3972\n",
      "Time for epoch 59 is 18.9352 sec\n",
      "Epoch :  60\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4052, disc_loss: -8.4185\n",
      "Time for epoch 60 is 18.7773 sec\n",
      "Epoch :  61\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.1253, disc_loss: -8.3383\n",
      "Time for epoch 61 is 21.7829 sec\n",
      "Epoch :  62\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.0830, disc_loss: -8.3768\n",
      "Time for epoch 62 is 19.1266 sec\n",
      "Epoch :  63\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7864, disc_loss: -8.4628\n",
      "Time for epoch 63 is 18.8267 sec\n",
      "Epoch :  64\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.2298, disc_loss: -8.3825\n",
      "Time for epoch 64 is 20.4289 sec\n",
      "Epoch :  65\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.2131, disc_loss: -8.3963\n",
      "Time for epoch 65 is 18.9603 sec\n",
      "Epoch :  66\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.6743, disc_loss: -8.4164\n",
      "Time for epoch 66 is 20.6526 sec\n",
      "Epoch :  67\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.1643, disc_loss: -8.3847\n",
      "Time for epoch 67 is 19.5587 sec\n",
      "Epoch :  68\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 55.0561, disc_loss: -8.3576\n",
      "Time for epoch 68 is 18.9617 sec\n",
      "Epoch :  69\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 27.1677, disc_loss: -8.3722\n",
      "Time for epoch 69 is 18.8282 sec\n",
      "Epoch :  70\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 22.4581, disc_loss: -8.3695\n",
      "Time for epoch 70 is 18.9812 sec\n",
      "Epoch :  71\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.4180, disc_loss: -8.4096\n",
      "Time for epoch 71 is 18.9842 sec\n",
      "Epoch :  72\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.2869, disc_loss: -8.3519\n",
      "Time for epoch 72 is 18.8625 sec\n",
      "Epoch :  73\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.9028, disc_loss: -8.3705\n",
      "Time for epoch 73 is 18.8348 sec\n",
      "Epoch :  74\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.8168, disc_loss: -8.3874\n",
      "Time for epoch 74 is 18.9792 sec\n",
      "Epoch :  75\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5563, disc_loss: -8.3935\n",
      "Time for epoch 75 is 19.1022 sec\n",
      "Epoch :  76\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.3878, disc_loss: -8.4311\n",
      "Time for epoch 76 is 18.8104 sec\n",
      "Epoch :  77\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5796, disc_loss: -8.3938\n",
      "Time for epoch 77 is 18.9255 sec\n",
      "Epoch :  78\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5448, disc_loss: -8.3942\n",
      "Time for epoch 78 is 18.9861 sec\n",
      "Epoch :  79\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.0415, disc_loss: -8.3682\n",
      "Time for epoch 79 is 19.0198 sec\n",
      "Epoch :  80\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.7188, disc_loss: -8.3883\n",
      "Time for epoch 80 is 19.1299 sec\n",
      "Epoch :  81\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.2564, disc_loss: -8.4296\n",
      "Time for epoch 81 is 18.8141 sec\n",
      "Epoch :  82\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.7399, disc_loss: -8.3893\n",
      "Time for epoch 82 is 18.9094 sec\n",
      "Epoch :  83\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.9986, disc_loss: -8.4306\n",
      "Time for epoch 83 is 18.8968 sec\n",
      "Epoch :  84\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.3196, disc_loss: -8.3739\n",
      "Time for epoch 84 is 18.7406 sec\n",
      "Epoch :  85\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.6645, disc_loss: -8.4331\n",
      "Time for epoch 85 is 18.6551 sec\n",
      "Epoch :  86\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5200, disc_loss: -8.4079\n",
      "Time for epoch 86 is 18.9845 sec\n",
      "Epoch :  87\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 41.8821, disc_loss: -8.3856\n",
      "Time for epoch 87 is 18.9312 sec\n",
      "Epoch :  88\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 85.1512, disc_loss: -8.3951\n",
      "Time for epoch 88 is 18.9896 sec\n",
      "Epoch :  89\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 52.0914, disc_loss: -8.3960\n",
      "Time for epoch 89 is 18.8937 sec\n",
      "Epoch :  90\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 36.9282, disc_loss: -8.4307\n",
      "Time for epoch 90 is 19.0455 sec\n",
      "Epoch :  91\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 30.7783, disc_loss: -8.4098\n",
      "Time for epoch 91 is 18.8767 sec\n",
      "Epoch :  92\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 28.1944, disc_loss: -8.4139\n",
      "Time for epoch 92 is 19.0686 sec\n",
      "Epoch :  93\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 26.5222, disc_loss: -8.3758\n",
      "Time for epoch 93 is 19.1581 sec\n",
      "Epoch :  94\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 25.5073, disc_loss: -8.3855\n",
      "Time for epoch 94 is 19.4052 sec\n",
      "Epoch :  95\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 24.8066, disc_loss: -8.3797\n",
      "Time for epoch 95 is 19.0868 sec\n",
      "Epoch :  96\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 22.8714, disc_loss: -8.4286\n",
      "Time for epoch 96 is 19.2699 sec\n",
      "Epoch :  97\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.8440, disc_loss: -8.4070\n",
      "Time for epoch 97 is 19.2769 sec\n",
      "Epoch :  98\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.3879, disc_loss: -8.4204\n",
      "Time for epoch 98 is 19.2482 sec\n",
      "Epoch :  99\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.4877, disc_loss: -8.4276\n",
      "Time for epoch 99 is 19.2608 sec\n",
      "Epoch :  100\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.4087, disc_loss: -8.3751\n",
      "Time for epoch 100 is 19.4943 sec\n",
      "Epoch :  101\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.8065, disc_loss: -8.3840\n",
      "Time for epoch 101 is 19.3174 sec\n",
      "Epoch :  102\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.2577, disc_loss: -8.4103\n",
      "Time for epoch 102 is 19.0378 sec\n",
      "Epoch :  103\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7960, disc_loss: -8.4278\n",
      "Time for epoch 103 is 19.0192 sec\n",
      "Epoch :  104\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.5276, disc_loss: -8.3836\n",
      "Time for epoch 104 is 19.0681 sec\n",
      "Epoch :  105\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7011, disc_loss: -8.4591\n",
      "Time for epoch 105 is 19.1095 sec\n",
      "Epoch :  106\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4205, disc_loss: -8.4231\n",
      "Time for epoch 106 is 19.2270 sec\n",
      "Epoch :  107\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.3539, disc_loss: -8.3856\n",
      "Time for epoch 107 is 19.1755 sec\n",
      "Epoch :  108\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7682, disc_loss: -8.4190\n",
      "Time for epoch 108 is 19.0220 sec\n",
      "Epoch :  109\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.1490, disc_loss: -8.4318\n",
      "Time for epoch 109 is 19.2125 sec\n",
      "Epoch :  110\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.2696, disc_loss: -8.3694\n",
      "Time for epoch 110 is 18.9890 sec\n",
      "Epoch :  111\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.8828, disc_loss: -8.4172\n",
      "Time for epoch 111 is 18.6300 sec\n",
      "Epoch :  112\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.9388, disc_loss: -8.4208\n",
      "Time for epoch 112 is 18.7332 sec\n",
      "Epoch :  113\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7647, disc_loss: -8.4054\n",
      "Time for epoch 113 is 18.9270 sec\n",
      "Epoch :  114\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.3669, disc_loss: -8.4485\n",
      "Time for epoch 114 is 18.7971 sec\n",
      "Epoch :  115\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 27.1879, disc_loss: -8.3503\n",
      "Time for epoch 115 is 18.8610 sec\n",
      "Epoch :  116\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5677, disc_loss: -8.3918\n",
      "Time for epoch 116 is 18.9826 sec\n",
      "Epoch :  117\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 33.0126, disc_loss: -8.4101\n",
      "Time for epoch 117 is 19.0150 sec\n",
      "Epoch :  118\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.6651, disc_loss: -8.4109\n",
      "Time for epoch 118 is 19.1234 sec\n",
      "Epoch :  119\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.0190, disc_loss: -8.4569\n",
      "Time for epoch 119 is 19.1644 sec\n",
      "Epoch :  120\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.4800, disc_loss: -8.4186\n",
      "Time for epoch 120 is 19.0447 sec\n",
      "Epoch :  121\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.6985, disc_loss: -8.4285\n",
      "Time for epoch 121 is 19.1729 sec\n",
      "Epoch :  122\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.8725, disc_loss: -8.4140\n",
      "Time for epoch 122 is 19.4155 sec\n",
      "Epoch :  123\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.3582, disc_loss: -8.4103\n",
      "Time for epoch 123 is 18.8599 sec\n",
      "Epoch :  124\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.9195, disc_loss: -8.4052\n",
      "Time for epoch 124 is 19.2761 sec\n",
      "Epoch :  125\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.0105, disc_loss: -8.4223\n",
      "Time for epoch 125 is 19.0631 sec\n",
      "Epoch :  126\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.2023, disc_loss: -8.4208\n",
      "Time for epoch 126 is 19.1927 sec\n",
      "Epoch :  127\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.2493, disc_loss: -8.4194\n",
      "Time for epoch 127 is 19.2833 sec\n",
      "Epoch :  128\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.0974, disc_loss: -8.4462\n",
      "Time for epoch 128 is 19.0750 sec\n",
      "Epoch :  129\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.1717, disc_loss: -8.4005\n",
      "Time for epoch 129 is 19.2225 sec\n",
      "Epoch :  130\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 29.6286, disc_loss: -8.3831\n",
      "Time for epoch 130 is 19.2700 sec\n",
      "Epoch :  131\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 52.2114, disc_loss: -8.4450\n",
      "Time for epoch 131 is 19.3105 sec\n",
      "Epoch :  132\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 23.2399, disc_loss: -8.3814\n",
      "Time for epoch 132 is 19.3531 sec\n",
      "Epoch :  133\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.9530, disc_loss: -8.4184\n",
      "Time for epoch 133 is 19.3004 sec\n",
      "Epoch :  134\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.0574, disc_loss: -8.4101\n",
      "Time for epoch 134 is 19.2125 sec\n",
      "Epoch :  135\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4003, disc_loss: -8.4921\n",
      "Time for epoch 135 is 19.3901 sec\n",
      "Epoch :  136\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.1531, disc_loss: -8.3964\n",
      "Time for epoch 136 is 19.1893 sec\n",
      "Epoch :  137\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.1176, disc_loss: -8.4035\n",
      "Time for epoch 137 is 19.0287 sec\n",
      "Epoch :  138\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.1353, disc_loss: -8.4828\n",
      "Time for epoch 138 is 19.1742 sec\n",
      "Epoch :  139\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 27.9622, disc_loss: -8.4370\n",
      "Time for epoch 139 is 19.2562 sec\n",
      "Epoch :  140\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.8984, disc_loss: -8.4524\n",
      "Time for epoch 140 is 19.2752 sec\n",
      "Epoch :  141\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4219, disc_loss: -8.4576\n",
      "Time for epoch 141 is 19.3553 sec\n",
      "Epoch :  142\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4979, disc_loss: -8.3812\n",
      "Time for epoch 142 is 19.7173 sec\n",
      "Epoch :  143\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.3890, disc_loss: -8.3870\n",
      "Time for epoch 143 is 19.3812 sec\n",
      "Epoch :  144\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.2509, disc_loss: -8.4407\n",
      "Time for epoch 144 is 19.3355 sec\n",
      "Epoch :  145\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.0514, disc_loss: -8.4230\n",
      "Time for epoch 145 is 19.4540 sec\n",
      "Epoch :  146\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.3395, disc_loss: -8.3986\n",
      "Time for epoch 146 is 19.3541 sec\n",
      "Epoch :  147\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.3900, disc_loss: -8.4236\n",
      "Time for epoch 147 is 19.6687 sec\n",
      "Epoch :  148\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.8671, disc_loss: -8.4345\n",
      "Time for epoch 148 is 19.4243 sec\n",
      "Epoch :  149\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.1655, disc_loss: -8.4724\n",
      "Time for epoch 149 is 19.5477 sec\n",
      "Epoch :  150\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.0305, disc_loss: -8.4340\n",
      "Time for epoch 150 is 19.7749 sec\n",
      "Epoch :  151\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.9303, disc_loss: -8.4254\n",
      "Time for epoch 151 is 19.8347 sec\n",
      "Epoch :  152\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.7395, disc_loss: -8.4486\n",
      "Time for epoch 152 is 18.9678 sec\n",
      "Epoch :  153\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.1773, disc_loss: -8.4257\n",
      "Time for epoch 153 is 19.0492 sec\n",
      "Epoch :  154\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 20.0956, disc_loss: -8.4146\n",
      "Time for epoch 154 is 19.2212 sec\n",
      "Epoch :  155\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.7545, disc_loss: -8.4244\n",
      "Time for epoch 155 is 19.3323 sec\n",
      "Epoch :  156\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4001, disc_loss: -8.4542\n",
      "Time for epoch 156 is 19.3615 sec\n",
      "Epoch :  157\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 47.3031, disc_loss: -8.4262\n",
      "Time for epoch 157 is 19.2705 sec\n",
      "Epoch :  158\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 27.1717, disc_loss: -8.4570\n",
      "Time for epoch 158 is 19.3365 sec\n",
      "Epoch :  159\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.6579, disc_loss: -8.4308\n",
      "Time for epoch 159 is 19.6035 sec\n",
      "Epoch :  160\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 19.8086, disc_loss: -8.4196\n",
      "Time for epoch 160 is 19.7724 sec\n",
      "Epoch :  161\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 40.7851, disc_loss: -8.4333\n",
      "Time for epoch 161 is 19.5625 sec\n",
      "Epoch :  162\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 37.5613, disc_loss: -8.4420\n",
      "Time for epoch 162 is 19.3168 sec\n",
      "Epoch :  163\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 24.9612, disc_loss: -8.4426\n",
      "Time for epoch 163 is 19.4048 sec\n",
      "Epoch :  164\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 21.6102, disc_loss: -8.4691\n",
      "Time for epoch 164 is 19.5805 sec\n",
      "Epoch :  165\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.4585, disc_loss: -8.4842\n",
      "Time for epoch 165 is 19.5004 sec\n",
      "Epoch :  166\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.6048, disc_loss: -8.4538\n",
      "Time for epoch 166 is 19.2930 sec\n",
      "Epoch :  167\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.9791, disc_loss: -8.4599\n",
      "Time for epoch 167 is 19.4477 sec\n",
      "Epoch :  168\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.1715, disc_loss: -8.4503\n",
      "Time for epoch 168 is 19.5484 sec\n",
      "Epoch :  169\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.8158, disc_loss: -8.4273\n",
      "Time for epoch 169 is 19.3486 sec\n",
      "Epoch :  170\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 16.7497, disc_loss: -8.4705\n",
      "Time for epoch 170 is 19.2979 sec\n",
      "Epoch :  171\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.9504, disc_loss: -8.4467\n",
      "Time for epoch 171 is 19.2769 sec\n",
      "Epoch :  172\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 18.1672, disc_loss: -8.4640\n",
      "Time for epoch 172 is 19.5560 sec\n",
      "Epoch :  173\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "gen_loss: 17.5326, disc_loss: -8.4667\n",
      "Time for epoch 173 is 20.5796 sec\n",
      "Epoch :  174\n",
      "0\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "train(dataset, hparas['N_EPOCH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6bef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c847e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
